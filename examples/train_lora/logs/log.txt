[2024-07-31 19:20:49,434] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
07/31/2024 19:20:52 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:20933
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
[2024-07-31 19:20:58,391] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-31 19:20:58,425] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-31 19:20:58,450] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-31 19:20:58,450] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-31 19:20:58,471] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-31 19:20:58,549] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-31 19:20:58,569] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-31 19:20:58,637] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-31 19:21:01,125] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-31 19:21:01,167] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-31 19:21:01,223] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-31 19:21:01,223] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-07-31 19:21:01,287] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-31 19:21:01,322] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-31 19:21:01,331] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-31 19:21:01,353] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-31 19:21:01,415] [INFO] [comm.py:637:init_distributed] cdb=None
07/31/2024 19:21:01 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
07/31/2024 19:21:01 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
07/31/2024 19:21:01 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
07/31/2024 19:21:01 - INFO - llamafactory.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
07/31/2024 19:21:01 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
07/31/2024 19:21:01 - INFO - llamafactory.hparams.parser - Process rank: 6, device: cuda:6, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
07/31/2024 19:21:01 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
07/31/2024 19:21:01 - INFO - llamafactory.hparams.parser - Process rank: 7, device: cuda:7, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
07/31/2024 19:21:01 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
07/31/2024 19:21:01 - INFO - llamafactory.hparams.parser - Process rank: 5, device: cuda:5, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
07/31/2024 19:21:01 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
07/31/2024 19:21:01 - INFO - llamafactory.hparams.parser - Process rank: 4, device: cuda:4, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
07/31/2024 19:21:01 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
07/31/2024 19:21:01 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2106] 2024-07-31 19:21:01,495 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2106] 2024-07-31 19:21:01,495 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2106] 2024-07-31 19:21:01,495 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2106] 2024-07-31 19:21:01,495 >> loading file tokenizer_config.json
07/31/2024 19:21:01 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
07/31/2024 19:21:01 - INFO - llamafactory.hparams.parser - Process rank: 3, device: cuda:3, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
07/31/2024 19:21:01 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
07/31/2024 19:21:01 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
07/31/2024 19:21:01 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
07/31/2024 19:21:01 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
07/31/2024 19:21:01 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
[WARNING|logging.py:314] 2024-07-31 19:21:01,819 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
07/31/2024 19:21:01 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
07/31/2024 19:21:01 - INFO - llamafactory.data.loader - Loading dataset wiki_demo.txt...
07/31/2024 19:21:01 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
07/31/2024 19:21:01 - INFO - llamafactory.data.template - Add pad token: <|end_of_text|>
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 200 examples [00:00, 2834.07 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/200 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 200/200 [00:00<00:00, 1089.34 examples/s]
07/31/2024 19:21:09 - INFO - llamafactory.data.loader - Loading dataset wiki_demo.txt...
07/31/2024 19:21:09 - INFO - llamafactory.data.loader - Loading dataset wiki_demo.txt...
07/31/2024 19:21:09 - INFO - llamafactory.data.loader - Loading dataset wiki_demo.txt...
07/31/2024 19:21:09 - INFO - llamafactory.data.loader - Loading dataset wiki_demo.txt...
07/31/2024 19:21:09 - INFO - llamafactory.data.loader - Loading dataset wiki_demo.txt...
07/31/2024 19:21:09 - INFO - llamafactory.data.loader - Loading dataset wiki_demo.txt...
07/31/2024 19:21:09 - INFO - llamafactory.data.loader - Loading dataset wiki_demo.txt...
Running tokenizer on dataset (num_proc=16):   0%|          | 0/200 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 13/200 [00:00<00:07, 26.35 examples/s]Running tokenizer on dataset (num_proc=16):  13%|█▎        | 26/200 [00:00<00:03, 49.53 examples/s]Running tokenizer on dataset (num_proc=16):  20%|█▉        | 39/200 [00:00<00:02, 55.73 examples/s]Running tokenizer on dataset (num_proc=16):  32%|███▎      | 65/200 [00:00<00:01, 100.03 examples/s]Running tokenizer on dataset (num_proc=16):  46%|████▌     | 91/200 [00:01<00:00, 111.22 examples/s]Running tokenizer on dataset (num_proc=16):  58%|█████▊    | 116/200 [00:01<00:00, 115.86 examples/s]Running tokenizer on dataset (num_proc=16):  70%|███████   | 140/200 [00:01<00:00, 116.76 examples/s]Running tokenizer on dataset (num_proc=16):  82%|████████▏ | 164/200 [00:01<00:00, 131.39 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 188/200 [00:01<00:00, 134.12 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 200/200 [00:02<00:00, 97.81 examples/s] 
training example:
input_ids:
[2127, 1132, 2191, 374, 264, 5054, 19675, 323, 7351, 430, 374, 67451, 950, 315, 11447, 323, 60515, 682, 91605, 11, 55363, 535, 7739, 315, 30022, 13, 1556, 1132, 2191, 6880, 369, 279, 76445, 315, 279, 1614, 11, 902, 433, 10187, 311, 387, 26225, 11, 77344, 11, 323, 28856, 13, 1666, 264, 35901, 2163, 29480, 7351, 11, 9277, 389, 279, 3117, 61943, 2163, 315, 279, 5054, 20326, 11, 433, 374, 6118, 7633, 16662, 57937, 2191, 323, 57125, 83815, 439, 279, 57125, 20611, 320, 2808, 531, 8997, 51618, 8, 315, 279, 41289, 7351, 11, 323, 706, 264, 3831, 13970, 15360, 449, 7294, 98231, 2191, 323, 51618, 3924, 372, 598, 12439, 304, 34775, 2085, 16287, 12694, 1132, 552, 1317, 1603, 279, 21967, 315, 16287, 5415, 11, 77563, 11, 477, 991, 19505, 13, 3161, 279, 10205, 315, 39433, 70994, 13162, 11, 67451, 42914, 9017, 11447, 1101, 16392, 13, 10541, 35483, 315, 78431, 3463, 527, 1766, 6957, 3925, 11, 6617, 44565, 2191, 22763, 505, 279, 92931, 13, 12220, 279, 15629, 4376, 315, 279, 220, 777, 339, 323, 279, 1176, 11026, 315, 279, 220, 508, 339, 9478, 11, 279, 78431, 7351, 20415, 3384, 304, 1455, 5596, 315, 279, 1917, 323, 1047, 264, 5199, 3560, 304, 7487, 6, 28970, 369, 91225, 49686, 13, 40741, 78431, 8853, 315, 3463, 14454, 2391, 420, 4261, 13, 1556, 1132, 1705, 617, 4529, 961, 304, 3892, 93574, 11, 1455, 35146, 304, 279, 12366, 6947, 2957, 11, 279, 8690, 16803, 5111, 323, 279, 15506, 16803, 5111, 11, 6832, 842, 13160, 279, 842, 315, 279, 29924, 11639, 315, 44565, 2191, 13, 763, 279, 1566, 11026, 315, 279, 220, 508, 339, 323, 1139, 279, 220, 1691, 267, 9478, 11, 279, 78431, 7351, 706, 1027, 594, 86153, 3131, 810, 8794, 1132, 2191, 51242, 264, 20057, 315, 26411, 304, 2015, 311, 3449, 1202, 10728, 10548, 902, 649, 387, 44029, 19180, 1139, 30191, 323, 41993, 26411, 26, 1070, 374, 5199, 28347, 1990, 279, 1403, 11, 902, 527, 16632, 53944, 13, 67679, 26411, 9395, 311, 4546, 1523, 11447, 323, 1614, 11, 3515, 4529, 264, 16806, 2543, 304, 279, 3347, 11, 1418, 41993, 26411, 9395, 311, 864, 18257, 1148, 459, 78431, 8396, 1053, 387, 1093, 13, 1556, 1132, 380, 3463, 11, 19347, 11, 323, 550, 7332, 617, 6476, 264, 961, 304, 17226, 5789, 315, 3823, 8396, 13, 34307, 42914, 315, 44565, 2191, 2997, 8349, 430, 433, 374, 34167, 40240, 11, 16806, 11, 477, 8791, 48748, 5253, 83, 99174, 11, 57726, 11, 323, 7419, 578, 1880, 1631, 5848, 6371, 315, 44565, 2191, 374, 505, 279, 38050, 18341, 459, 847, 71, 689, 11, 7438, 330, 30096, 264, 49080, 498, 24306, 315, 279, 9436, 459, 12, 3573, 30096, 909, 323, 279, 3492, 802, 31764, 437, 3573, 38491, 1, 477, 330, 81, 8646, 1865, 578, 21166, 482, 2191, 72214, 279, 42933, 1510, 430, 9428, 2530, 459, 15630, 13, 1556, 1132, 2191, 8111, 304, 6498, 505, 220, 10513, 17, 439, 44565, 44618, 323, 459, 15630, 505, 220, 9800, 24, 26, 4216, 6498, 603, 1154, 20654, 4147, 264, 5647, 315, 19823, 13, 40741, 48752, 2949, 279, 8753, 22910, 61336, 872, 19949, 439, 93134, 11, 8051, 2478, 1778, 13487, 6222, 1690, 6325, 449, 3010, 93134, 13, 9176, 14110, 5548, 315, 279, 220, 777, 339, 9478, 1778, 439, 12656, 4359, 7678, 320, 10005, 21, 4235, 10750, 21, 8, 323, 93537, 1226, 275, 2785, 320, 5245, 23, 4235, 9674, 16, 8, 1053, 17210, 311, 279, 78431, 83258, 315, 279, 1828, 9659, 719, 1550, 539, 1005, 78431, 477, 44565, 2191, 304, 23524, 5694, 477, 872, 21463, 11829, 1176, 5054, 55475, 311, 1650, 5678, 459, 78431, 1754, 574, 38077, 12278, 974, 764, 393, 583, 31721, 263, 320, 5245, 24, 4235, 9714, 20, 705, 36024, 279, 16287, 7342, 315, 44565, 2191, 304, 279, 5209, 12, 777, 339, 9478, 13, 8876, 279, 220, 9378, 15, 82, 323, 7314, 304, 9822, 11, 57125, 2191, 706, 3629, 1027, 1511, 439, 264, 74450, 369, 44565, 2191, 323, 1202, 1005, 439, 264, 74450, 374, 2103, 4279, 4994, 279, 3723, 4273, 13, 4427, 603, 1154, 315, 57125, 2191, 8464, 311, 3927, 4633, 1949, 48831, 19675, 1193, 11, 323, 1949, 48831, 44565, 2191, 304, 4040, 374, 61937, 57125, 44565, 2191, 95386, 279, 4751, 57125, 706, 1027, 14090, 69593, 449, 44565, 2191, 11, 1202, 7438, 706, 810, 6051, 80703, 449, 22622, 25375, 505, 2679, 30450, 85129, 5315, 11, 2737, 2225, 279, 1561, 14043, 323, 57125, 28187, 1705, 11, 889, 656, 539, 22712, 5694, 449, 59021, 3674, 1705, 477, 264, 348, 53290, 4717, 11, 323, 14560, 13042, 45750, 11, 889, 527, 15871, 11920, 449, 8431, 58455, 13, 23212, 11, 1063, 93134, 1005, 57125, 41289, 311, 5766, 44565, 2191, 596, 8389, 390, 14632, 323, 20654, 1082, 1202, 13537, 449, 51618, 13, 1556, 1132, 2191, 374, 44029, 1511, 311, 7664, 279, 7294, 43802, 20631, 20611, 315, 279, 41289, 7351, 13, 1556, 1132, 2191, 374, 13168, 291, 311, 41289, 7739, 902, 527, 1614, 36185, 477, 505, 3485, 13, 99394, 315, 44565, 2191, 8965, 11415, 44565, 2191, 596, 41289, 16792, 323, 9940, 1082, 13865, 520, 6968, 29953, 354, 316, 552, 1990, 279, 1403, 13, 4427, 31839, 7664, 44565, 2191, 439, 3515, 1690, 34453, 505, 84581, 11, 323, 1694, 2225, 45750, 323, 3674, 1705, 719, 810, 779, 11, 1418, 1455, 31839, 8007, 459, 277, 971, 98231, 2191, 439, 264, 70847, 315, 78431, 16565, 95386, 14076, 311, 279, 1614, 374, 8792, 311, 78431, 3463, 11, 27409, 44565, 2191, 374, 539, 459, 4228, 3465, 369, 31839, 11, 439, 1070, 374, 264, 2763, 315, 10430, 4315, 31839, 323, 93134, 389, 279, 5030, 11, 323, 5370, 60701, 45493, 44565, 2191, 10284, 22009, 13, 17559, 36222, 3079, 5540, 2997, 279, 690, 369, 264, 2536, 23283, 3035, 535, 8396, 11, 279, 38001, 315, 279, 1614, 41705, 11, 279, 16801, 430, 3823, 7138, 6276, 12966, 311, 3073, 304, 477, 5208, 9017, 1778, 264, 2536, 23283, 3035, 535, 8396, 11, 323, 264, 24710, 389, 1268, 311, 1180, 311, 23564, 279, 10728, 315, 459, 15630, 3924, 2644, 4808, 17515, 944, 11639, 13538, 279, 21967, 315, 25861, 323, 9919, 11, 459, 9749, 11447, 1550, 539, 3073, 13, 1102, 574, 1306, 279, 9886, 315, 14673, 315, 11447, 430, 44565, 4633, 6848, 16948, 37588, 439, 264, 13010, 13, 578, 1455, 28289, 5956, 34291, 311, 44565, 2191, 304, 279, 14154, 1917, 1051, 304, 5734, 323, 25431, 13, 763, 5734, 11, 41903, 44565, 2191, 320, 1820, 10430, 389, 279, 57008, 315, 279, 1614, 8, 574, 91784, 660, 555, 60608, 380, 61787, 68844, 526, 67927, 323, 445, 3524, 8510, 13, 32944, 3002, 71883, 42914, 11, 60608, 2191, 706, 1027, 1071, 311, 617, 1047, 330, 91645, 16961, 811, 1, 315, 44565, 2191, 13, 1556, 1132, 292, 33726, 1051, 1101, 83280, 555, 28375, 5493, 323, 61787, 304, 25431, 13, 362, 60478, 4010, 355, 323, 34940, 511, 645, 1511, 279, 21849, 315, 6898, 343, 606, 311, 41468, 279, 12324, 1990, 5718, 743, 555, 279, 1614, 323, 4443, 51360, 13, 328, 78046, 29440, 59652, 1122, 11527, 15320, 323, 29676, 389, 279, 1314, 315, 3927, 11542, 315, 42563, 13, 356, 1910, 1233, 27292, 3823, 2383, 320, 17101, 437, 8, 323, 5938, 11527, 1418, 4560, 311, 3974, 4184, 311, 7138, 320, 764, 4548, 570, 71883, 1233, 1051, 33445, 315, 264, 8396, 3196, 389, 57751, 323, 11919, 4398, 4315, 1202, 10495, 2085, 279, 9546, 315, 264, 1614, 5450, 42108, 4606, 11, 1070, 574, 912, 44565, 4633, 5820, 3734, 1063, 14943, 5411, 10597, 19567, 13, 4314, 11, 323, 1023, 10451, 19567, 11, 3010, 6688, 7342, 311, 10597, 44565, 2191, 13, 763, 279, 328, 46488, 1122, 21080, 11, 40091, 67, 587, 2663, 369, 459, 77271, 20631, 8396, 323, 279, 76445, 315, 87149, 11, 1193, 311, 387, 5246, 16070, 555, 35414, 735, 38155, 358, 5450, 15004, 969, 11, 10597, 31237, 82, 89194, 2403, 279, 1614, 13, 763, 4606, 11, 5370, 31237, 82, 8040, 7294, 21395, 323, 57125, 61555, 13, 50086, 291, 2802, 304, 61386, 488, 2391, 279, 55383, 323, 304, 879, 19971, 2391, 279, 1050, 1659, 28101, 5540, 315, 7294, 43802, 20631, 37019, 2191, 11, 8104, 304, 9822, 13, 92931, 11774, 311, 20207, 11447, 320, 5132, 1299, 323, 10597, 8, 323, 279, 93574, 315, 279, 220, 11128, 15, 82, 323, 220, 10336, 23, 682, 85747, 279, 42933, 4500, 315, 1148, 6244, 279, 11639, 315, 29924, 44565, 2191, 24002, 944, 11639, 12220, 279, 8753, 22910, 11, 49638, 5315, 1778, 439, 279, 2998, 4193, 5512, 323, 279, 220, 5602, 264, 13353, 1486, 304, 279, 74454, 315, 7294, 21395, 323, 6918, 380, 58214, 13, 578, 1176, 78431, 60701, 8040, 6957, 279, 220, 972, 339, 9478, 439, 12656, 4359, 7678, 16948, 37588, 41903, 44565, 2191, 304, 9635, 11, 57323, 20445, 275, 318, 3876, 279, 1614, 11, 7639, 65292, 1215, 596, 7422, 63675, 279, 1648, 311, 3927, 2191, 323, 38077, 12278, 974, 764, 393, 583, 31721, 263, 596, 10334, 315, 27848, 2191, 1766, 70225, 17614, 304, 9822, 13, 3296, 279, 3389, 220, 9674, 15, 82, 11, 5370, 78431, 8853, 315, 3463, 1047, 3719, 1664, 39817, 323, 264, 12330, 315, 1243, 31069, 3728, 8082, 10222, 505, 220, 9367, 15, 311, 220, 7529, 19, 13, 1115, 11639, 315, 29924, 44565, 2191, 36513, 3156, 279, 842, 315, 279, 15506, 16803, 5111, 323, 374, 6646, 279, 21411, 4325, 315, 44565, 2191, 1852, 505, 27848, 2191, 11, 92551, 36769, 359, 258, 18538, 6667, 80244, 44565, 2191, 323, 10862, 279, 7327, 22938, 5794, 596, 10229, 11, 264, 538, 12128, 11552, 3010, 3967, 439, 279, 5629, 7327, 430, 14454, 304, 220, 9714, 19, 311, 52696, 17226, 30191, 60701, 13, 578, 7327, 6244, 264, 5199, 5054, 5457, 11, 449, 35131, 28187, 1694, 264, 6522, 7216, 323, 264, 4562, 315, 1202, 3331, 9251, 13, 36769, 359, 258, 596, 37480, 320, 1820, 622, 5808, 28331, 8, 323, 393, 583, 31721, 263, 596, 20723, 320, 1820, 27848, 1705, 8, 16475, 1614, 51618, 11, 59416, 5054, 63944, 3012, 2191, 323, 2678, 3424, 58348, 13, 4740, 26242, 42254, 11, 279, 36769, 359, 258, 1705, 1051, 67331, 505, 279, 7327, 555, 279, 28187, 1705, 520, 279, 220, 9674, 17, 86026, 8151, 13, 1556, 1132, 1705, 1051, 12020, 30293, 304, 279, 10657, 7327, 11, 1694, 13967, 67331, 304, 220, 9378, 21, 13, 36769, 359, 258, 51287, 19698, 430, 422, 14110, 5548, 18661, 2410, 555, 28187, 596, 3878, 11, 814, 1053, 842, 709, 279, 502, 43049, 1821, 315, 7487, 13, 763, 2077, 311, 872, 95989, 505, 279, 5629, 7327, 11, 93134, 14454, 279, 800, 13, 2417, 1291, 7327, 13, 9636, 279, 10383, 315, 11291, 735, 897, 354, 8148, 11, 264, 8690, 55475, 323, 28568, 11, 459, 277, 971, 88389, 359, 2191, 29204, 5795, 449, 6667, 74050, 13, 1556, 277, 971, 88389, 359, 1705, 11, 889, 24465, 20343, 505, 279, 220, 9674, 16, 12366, 6947, 2957, 11, 64854, 369, 1949, 80375, 323, 369, 279, 8141, 315, 11822, 4184, 311, 832, 596, 3966, 15913, 279, 2543, 315, 279, 9478, 11, 44565, 2191, 1047, 9041, 682, 927, 279, 1917, 13, 1102, 574, 264, 28289, 4668, 315, 279, 6625, 22013, 950, 2191, 7351, 13, 763, 5734, 11, 2678, 5315, 315, 4236, 25973, 279, 3823, 4633, 463, 31419, 1873, 2373, 315, 459, 277, 971, 88389, 359, 2191, 13, 27286, 574, 264, 80310, 369, 42301, 1245, 12822, 505, 5961, 315, 279, 3117, 11226, 11, 31829, 311, 279, 11002, 6864, 311, 4007, 13, 763, 20023, 5270, 11, 32164, 574, 264, 86568, 369, 459, 277, 971, 1355, 88, 303, 950, 2191, 11, 1405, 433, 6244, 279, 1455, 21102, 2163, 29480, 34649, 13, 12220, 420, 892, 11, 264, 23413, 315, 93134, 18306, 26411, 315, 30191, 5054, 9349, 13, 1115, 8446, 6244, 3967, 439, 30617, 315, 279, 56408, 13, 578, 834, 9792, 479, 315, 279, 8753, 41289, 7351, 1139, 1690, 5315, 323, 279, 11572, 323, 61087, 315, 1690, 57298, 2402, 311, 47426, 49028, 2768, 279, 46735, 315, 279, 12366, 6947, 2957, 92867, 3927, 380, 5054, 7645, 323, 14385, 13, 7570, 3582, 1690, 93134, 1612, 4979, 5694, 505, 1521, 20320, 14385, 11, 4225, 27322, 3782, 5304, 279, 7351, 323, 13865, 1051, 1903, 311, 22429, 1124, 505, 3778, 15443, 11, 2737, 279, 40782, 3298, 315, 220, 7028, 18, 11, 1101, 2663, 279, 1556, 1132, 380, 1398, 9134, 3298, 13, 15388, 2191, 574, 2500, 8446, 902, 1063, 93134, 18306, 2391, 420, 4261, 24458, 6100, 10742, 11, 93134, 99325, 31408, 304, 279, 8690, 22910, 304, 14076, 311, 279, 5929, 7351, 26, 4869, 11, 814, 2322, 25984, 46735, 1306, 279, 92501, 3109, 574, 27276, 4147, 13, 26778, 93134, 505, 62579, 6902, 323, 23223, 30010, 311, 19278, 11, 35146, 6522, 311, 279, 97660, 45378, 53848, 323, 40005, 269, 386, 22506, 2201, 596, 14993, 304, 279, 3658, 49187, 13, 3161, 279, 93134, 1694, 33745, 304, 8524, 11, 1403, 502, 3276, 411, 45608, 60701, 22763, 11, 32125, 5452, 2191, 323, 39975, 44565, 2191, 13, 578, 4846, 16495, 311, 1893, 264, 56887, 1912, 430, 1053, 4585, 369, 14110, 1418, 279, 15629, 1051, 2403, 4205, 430, 1053, 52280, 264, 5054, 4717, 13, 56124, 279, 46146, 315, 279, 92501, 82, 304, 279, 6664, 22910, 323, 279, 13239, 8690, 16803, 5111, 11, 1690, 7487, 323, 21572, 6656, 311, 50315, 9875, 902, 14264, 520, 279, 20900, 315, 44565, 2191, 323, 1023, 41289, 19567, 13, 763, 9822, 323, 279, 3723, 4273, 11, 3697, 315, 3682, 22013, 950, 380, 19567, 1778, 439, 279, 3331, 39144, 367, 315, 18993, 323, 279, 25563, 36798, 315, 279, 4435, 2163, 872, 29533, 323, 11096, 279, 37961, 7327, 5450, 279, 15506, 16803, 5111, 315, 220, 7285, 21, 11, 93134, 323, 22013, 950, 1705, 320, 98354, 323, 15358, 40, 8, 3131, 1578, 54502, 5694, 449, 5370, 60701, 315, 2163, 1705, 13, 362, 1317, 14135, 315, 15506, 44565, 2191, 6197, 311, 93134, 5737, 264, 60850, 3560, 304, 279, 4208, 13, 763, 2077, 311, 279, 13695, 53848, 11, 459, 78431, 53161, 7351, 315, 76847, 323, 7487, 11, 7396, 555, 17903, 80592, 11, 3952, 2585, 315, 28035, 323, 315, 3544, 5789, 315, 19624, 18157, 11, 1405, 814, 6667, 344, 4147, 279, 4363, 13, 578, 19953, 9323, 3984, 1063, 7347, 13291, 520, 279, 7314, 315, 279, 4208, 11, 719, 279, 1121, 574, 264, 26242, 4465, 4315, 34561, 1705, 323, 93134, 520, 264, 4101, 315, 4455, 7086, 3297, 21882, 439, 15466, 47765, 6818, 311, 51085, 2585, 315, 279, 13063, 24336, 48260, 11639, 2468, 279, 842, 315, 4435, 5111, 8105, 11, 279, 78431, 7351, 574, 35906, 58764, 13, 578, 220, 5162, 15, 82, 32126, 264, 57105, 315, 44565, 2191, 11, 4461, 9057, 555, 264, 26617, 8060, 315, 83815, 4235, 11522, 258, 2191, 323, 39510, 5918, 555, 279, 24062, 5111, 13, 12220, 420, 892, 11, 44565, 2191, 1766, 264, 9546, 304, 1023, 19567, 9200, 7119, 2225, 32682, 323, 279, 1614, 1778, 439, 279, 7294, 5392, 9834, 11, 12434, 11, 323, 9096, 19567, 11, 279, 1797, 3035, 10745, 315, 279, 220, 5162, 15, 82, 11, 323, 279, 1561, 14043, 13, 1102, 1101, 5602, 264, 9320, 505, 1202, 3766, 30191, 7138, 311, 66998, 7294, 98231, 380, 15180, 2191, 13, 1556, 1132, 2191, 6244, 5938, 449, 36858, 1207, 70905, 439, 39039, 1908, 555, 21562, 1778, 439, 4656, 395, 323, 279, 6834, 45596, 3145, 13, 578, 9749, 38012, 61555, 315, 44565, 64, 2269, 26768, 2191, 6052, 449, 21466, 414, 2391, 279, 2132, 12330, 315, 53110, 13, 5348, 44565, 2191, 6137, 311, 1935, 1376, 520, 420, 892, 323, 28160, 44565, 2191, 596, 3351, 505, 264, 20026, 1189, 2265, 38462, 13, 1115, 23828, 4591, 449, 1202, 8060, 311, 8895, 47955, 304, 17355, 4606, 323, 1202, 31069, 2673, 304, 20023, 5270, 885, 1067, 279, 2543, 315, 279, 220, 1691, 267, 9478, 11, 44565, 2191, 14264, 304, 23354, 323, 10383, 2949, 7294, 98231, 380, 11, 7294, 48260, 323, 7294, 74419, 8082, 19567, 13, 1556, 1132, 1705, 6244, 3967, 369, 872, 22315, 304, 22670, 2403, 279, 4435, 17657, 21021, 320, 54, 5319, 705, 279, 5856, 315, 36944, 323, 279, 4435, 23362, 17997, 13, 12220, 279, 22670, 11, 1008, 67490, 7808, 1752, 22939, 19973, 417, 3967, 439, 3776, 41840, 82, 17045, 304, 42597, 287, 11, 3424, 19814, 323, 16806, 17302, 811, 449, 279, 4379, 13, 7089, 21874, 1697, 26411, 96734, 304, 420, 892, 2997, 51552, 5315, 11, 4868, 7829, 323, 279, 1005, 315, 39205, 4147, 14645, 1778, 439, 279, 8191, 13, 362, 5199, 1567, 315, 420, 4261, 574, 279, 17302, 811, 520, 279, 220, 2550, 24, 16759, 100233, 10017, 13, 1556, 1132, 380, 6848, 617, 1027, 32549, 304, 279, 4500, 315, 279, 1901, 26844, 27771, 304, 12550, 323, 279, 11650, 28331, 315, 17355, 12911, 11, 810, 17037, 3967, 439, 12093, 10248, 11, 264, 409, 61596, 39293, 5654, 304, 18671, 12911, 5221, 2509, 1556, 1132, 380, 8853, 315, 3463, 617, 1027, 8965, 41141, 1139, 1403, 1925, 13970, 32006, 11, 3674, 44565, 2191, 323, 3927, 380, 44565, 2191, 11, 56612, 311, 872, 2204, 33472, 11, 2819, 323, 15740, 13, 578, 3927, 380, 1510, 20654, 5014, 8389, 31220, 304, 31322, 312, 7610, 5304, 279, 1949, 3927, 11, 1418, 279, 3674, 1510, 20654, 5014, 6928, 31220, 304, 38178, 311, 11322, 279, 1949, 4754, 315, 8396, 1555, 22526, 323, 3674, 15637, 13, 763, 264, 87634, 5647, 11, 44565, 2191, 649, 387, 86045, 555, 279, 29924, 60701, 315, 279, 3389, 220, 777, 339, 9478, 323, 279, 1772, 15144, 950, 60701, 320, 276, 1132, 64, 2269, 26768, 2191, 11, 6307, 44565, 2191, 11, 323, 1772, 19415, 1132, 2191, 8, 8040, 46095, 1823, 23478, 279, 3230, 48752, 315, 78431, 19567, 902, 35256, 5054, 44565, 2191, 15812, 41903, 44565, 2191, 902, 10187, 430, 279, 1614, 37856, 16033, 57008, 11, 2085, 14647, 25694, 279, 48696, 315, 14110, 311, 22472, 433, 13, 362, 3777, 5423, 315, 3927, 380, 44565, 2191, 11, 41903, 44565, 2191, 1253, 50134, 279, 14209, 315, 264, 17832, 1614, 719, 8349, 430, 10495, 617, 912, 16033, 29672, 311, 41701, 3109, 994, 433, 26885, 449, 3927, 51360, 13, 1556, 1132, 2191, 21935, 5199, 6666, 311, 16033, 6105, 2533, 32008, 617, 264, 8792, 3560, 304, 78431, 19675, 13, 1556, 1132, 2191, 596, 25679, 389, 7294, 98231, 2191, 11, 77271, 20631, 2191, 11, 323, 369, 279, 9070, 315, 4029, 323, 3927, 488, 7437, 433, 10980, 505, 459, 277, 971, 98231, 2191, 323, 1023, 4595, 315, 7100, 57125, 2191, 8794, 1132, 2191, 374, 6118, 9277, 389, 279, 3117, 8109, 315, 279, 5054, 20326, 13, 24191, 315, 1202, 28989, 323, 5897, 19675, 8881, 7294, 43802, 20631, 11, 7294, 73454, 380, 11, 57125, 11, 323, 18336, 58689, 315, 2163, 29480, 323, 41289, 11759, 1778, 439, 6667, 74050, 11, 71189, 11, 3927, 2191, 11, 27848, 2191, 11, 323, 22013, 950, 2191, 11, 4315, 1023, 57125, 41289, 7100, 26018, 13, 1666, 44565, 2191, 1587, 539, 3085, 264, 8521, 2547, 315, 33235, 505, 264, 3254, 4040, 78162, 11, 1690, 78431, 4595, 323, 32006, 3073, 323, 36680, 315, 459, 15630, 37441, 713, 13882, 13, 3861, 13010, 2403, 79880, 2191, 2949, 279, 78431, 83259, 574, 44565, 2191, 2085, 1008, 87678, 11, 264, 1650, 369, 15230, 367, 323, 31426, 4315, 93134, 1176, 18306, 555, 51485, 350, 1138, 4849, 1624, 386, 1995, 8892, 337, 304, 220, 9367, 24, 304, 2077, 311, 279, 26242, 37635, 315, 78431, 10334, 520, 279, 892, 13, 7984, 4843, 304, 5054, 99172, 2191, 706, 1027, 16948, 37588, 555, 93134, 13, 18185, 25768, 11, 279, 5370, 78431, 8853, 315, 3463, 527, 539, 3970, 439, 12742, 15086, 719, 4856, 439, 61555, 430, 958, 76, 2222, 323, 527, 8599, 1555, 264, 743, 315, 14113, 16565, 1778, 439, 3927, 323, 2254, 51360, 11, 27848, 12576, 11, 4009, 22139, 11, 57937, 20095, 11, 35516, 11447, 323, 39205, 8082, 19891, 950, 763, 59374, 60701, 4315, 29924, 78431, 60701, 1051, 27848, 2191, 323, 3927, 2191, 13, 2435, 1051, 8272, 555, 279, 3682, 60701, 315, 3674, 44565, 2191, 320, 17840, 80244, 11, 50315, 323, 22013, 950, 380, 570, 2435, 1782, 389, 21874, 1697, 323, 7100, 13878, 315, 872, 10728, 8396, 1345, 332, 940, 2191, 374, 459, 220, 972, 339, 34457, 7100, 10334, 430, 574, 8040, 1139, 78431, 10334, 555, 38077, 12278, 974, 764, 393, 583, 31721, 263, 13, 11699, 22262, 2997, 67642, 9103, 11, 1949, 15360, 11, 37079, 5226, 11, 80375, 323, 33384, 15180, 315, 2225, 6807, 323, 11667, 430, 1053, 387, 35319, 555, 264, 6201, 315, 279, 1274, 13, 75142, 2191, 706, 1027, 77653, 3210, 3752, 4147, 439, 2679, 30450, 31183, 1990, 3927, 380, 323, 6667, 80244, 7739, 315, 44565, 2191, 13, 763, 3639, 2209, 8825, 30, 320, 10336, 15, 705, 393, 583, 31721, 263, 1176, 3752, 4147, 813, 5915, 439, 264, 330, 32827, 1376, 315, 8396, 11, 279, 39975, 315, 71189, 323, 3424, 1210, 21153, 80244, 44565, 2191, 374, 264, 30191, 41289, 1376, 315, 44565, 2191, 17037, 5938, 449, 92551, 36769, 359, 258, 13, 21153, 80244, 93134, 29115, 22498, 15637, 315, 279, 3445, 315, 5788, 902, 374, 46820, 4147, 311, 387, 17427, 1555, 16806, 14110, 323, 430, 7487, 387, 7318, 4184, 311, 892, 6575, 11, 4856, 1109, 11822, 1694, 4332, 4184, 311, 1205, 439, 304, 71189, 13, 21153, 80244, 44565, 2191, 51063, 16662, 83815, 719, 18010, 279, 69948, 315, 279, 88347, 8994, 279, 11224, 70050, 5915, 315, 264, 6667, 80244, 1614, 1752, 8396, 8794, 277, 971, 88389, 359, 2191, 374, 264, 10334, 315, 44565, 2191, 430, 28424, 264, 50315, 8396, 449, 4279, 15637, 315, 279, 3445, 315, 5788, 11, 2167, 20095, 323, 264, 16600, 4009, 315, 37079, 30257, 11, 7487, 6, 61783, 323, 12128, 22415, 5983, 11, 449, 5788, 323, 15652, 3196, 389, 279, 51346, 17966, 330, 3915, 1855, 4184, 311, 813, 5845, 11, 311, 1855, 4184, 311, 813, 1205, 1210, 1556, 277, 971, 88389, 359, 2191, 8040, 505, 18336, 41289, 60701, 1306, 279, 8753, 22910, 719, 574, 1176, 60394, 439, 1778, 304, 279, 15155, 3857, 315, 279, 5629, 7327, 13, 1102, 574, 3010, 17626, 5304, 304, 279, 32887, 990, 315, 11291, 735, 897, 354, 8148, 11, 6832, 3230, 1742, 1053, 733, 8800, 3719, 279, 68366, 1684, 315, 93134, 555, 279, 3389, 220, 777, 339, 9478, 13, 1556, 277, 971, 1355, 88, 303, 950, 2191, 374, 264, 9046, 315, 44565, 2191, 430, 6325, 23791, 22013, 25858, 439, 264, 4754, 5457, 369, 30191, 3674, 2349, 11, 25935, 32682, 323, 279, 1614, 449, 264, 502, 8396, 12345, 7167, 659, 21110, 3359, 555, 7487, 13, 578, 6913, 16565, 315, 459, 277, 971, 1355, 88, 303, 950, 2191, 527, 2167, 1957, 11, 7487, 6, 44254, 323, 7487, 6, 659, 55885, 13, 44259, 380, 44565, 2191, 374, 264, 743, 315, 3892, 32006, 315, 3463, 2949, 279, 78431, 7351, 430, 20654, 1082, 279, 3927, 323, 872, 690, 927, 904, 13124, 315, 9434, 6449, 1821, 13, 23591, 34453, 389, 3927, 380, 7739, 315, 44565, 2191, 2997, 12656, 4359, 7678, 11, 7639, 65292, 1215, 11, 323, 18063, 6941, 666, 461, 2933, 13, 17331, 1690, 5961, 11, 3927, 380, 44565, 2191, 29123, 264, 2678, 3686, 17226, 2768, 315, 52450, 336, 1122, 13820, 323, 83936, 439, 1664, 439, 3995, 78431, 704, 68637, 304, 1148, 6244, 3967, 439, 12079, 2191, 323, 3927, 312, 34084, 24336, 15144, 950, 323, 19225, 1556, 1132, 380, 16565, 1234, 70, 2668, 19225, 18336, 3674, 19567, 315, 279, 2163, 13, 25074, 304, 279, 78431, 7351, 8040, 16662, 24151, 304, 279, 7294, 74419, 8082, 7351, 11, 6832, 6522, 28941, 14488, 1051, 78431, 304, 17140, 13, 1666, 279, 7351, 27367, 220, 1691, 267, 9478, 18336, 2191, 11, 22622, 27830, 315, 78431, 16565, 75848, 264, 57105, 315, 2802, 13, 1556, 1132, 2191, 706, 8738, 311, 7068, 1690, 13868, 552, 323, 19567, 11, 520, 3115, 78813, 11, 13633, 5304, 5370, 8336, 323, 35271, 85129, 19476, 311, 1893, 502, 41903, 20414, 13, 578, 7294, 98231, 380, 14135, 315, 29924, 44565, 2191, 706, 14958, 21102, 2949, 19225, 60701, 2510, 14084, 3754, 10401, 902, 66159, 3776, 41840, 44895, 706, 49680, 44565, 2191, 596, 13970, 15360, 449, 28013, 323, 9349, 13, 11699, 43763, 706, 1101, 6197, 810, 31839, 304, 5151, 1778, 439, 95044, 323, 3925, 311, 16988, 449, 279, 78431, 7351, 11, 8051, 19225, 44565, 2191, 9428, 2530, 6299, 927, 14584, 10334, 13, 40741, 78431, 5315, 11, 61555, 11, 323, 8853, 315, 3463, 3073, 3432, 11, 3339, 433, 5107, 311, 7664, 279, 19225, 78431, 7351, 13, 6104, 84062, 323, 21572, 617, 9749, 330, 3833, 8046, 15528, 738, 616, 811, 315, 78431, 16565, 498, 1070, 374, 912, 24811, 389, 902, 16565, 527, 6332, 323, 63594, 7664, 5361, 44565, 13978, 11, 4856, 1109, 264, 35044, 44565, 2191, 11, 304, 902, 4279, 16565, 527, 6222, 1990, 8853, 315, 44565, 2191, 1418, 1855, 1912, 45777, 4861, 1884, 16565, 22009, 13, 29317, 22526, 649, 387, 264, 4279, 17966, 11, 8051, 433, 21467, 439, 264, 5190, 10844, 311, 44565, 64, 2269, 26768, 1705, 1109, 459, 277, 971, 88389, 359, 1705, 8794, 1132, 1705, 527, 8965, 11411, 2403, 55363, 535, 11447, 304, 682, 7739, 11, 32125, 330, 543, 58983, 323, 70994, 7739, 315, 3109, 320, 68, 1326, 2637, 87149, 11, 18740, 20095, 11, 1614, 51618, 11, 5099, 25390, 7100, 538, 6067, 320, 68, 1326, 2637, 32682, 11, 68357, 81913, 2191, 11, 95450, 2191, 11, 33792, 11, 5099, 25390, 3154, 38341, 44230, 320, 68, 1326, 2637, 16188, 380, 15256, 11, 13041, 16879, 2191, 11, 5099, 25390, 29325, 15630, 11, 30548, 20377, 2191, 11, 4251, 65503, 11, 323, 80051, 1210, 1556, 1132, 380, 8853, 29395, 389, 279, 5528, 555, 902, 1521, 7739, 1288, 387, 16475, 13, 578, 17966, 315, 6273, 31220, 374, 12401, 311, 78431, 5054, 32008, 304, 430, 433, 42590, 1438, 2225, 279, 18250, 323, 41289, 32006, 13, 1115, 71204, 430, 31220, 323, 22526, 4250, 387, 11798, 2949, 279, 1614, 11, 13239, 304, 279, 34685, 315, 682, 7739, 315, 55949, 323, 30022, 844, 69069, 1556, 1132, 1705, 6, 26411, 1935, 5370, 7739, 719, 304, 4689, 8854, 1403, 3682, 9021, 11, 32125, 311, 1176, 34134, 279, 87865, 323, 2132, 398, 311, 12192, 78431, 32008, 323, 8881, 459, 78431, 11376, 315, 8396, 11, 94012, 279, 31426, 315, 3445, 323, 10548, 13, 362, 7353, 22824, 8082, 649, 387, 1903, 1990, 22262, 311, 7066, 78612, 5415, 323, 14673, 555, 30191, 3445, 389, 832, 1450, 323, 22262, 311, 2349, 8396, 1555, 41993, 3445, 389, 279, 1023, 13, 38321, 661, 26411, 27830, 2536, 56251, 768, 11, 8007, 9349, 323, 1935, 264, 53722, 5603, 311, 78431, 22262, 11, 8051, 1070, 374, 5199, 28347, 1990, 279, 1403, 8794, 1132, 380, 26411, 617, 30073, 2391, 279, 3388, 315, 279, 1566, 9478, 13, 1556, 1132, 1705, 2391, 279, 4216, 220, 508, 339, 9478, 10968, 810, 389, 23170, 323, 5440, 6709, 1418, 19225, 93134, 1005, 264, 27927, 1358, 315, 20414, 19891, 950, 11639, 26411, 12220, 279, 29924, 11639, 11, 93134, 1047, 264, 52382, 31954, 13, 2876, 1193, 1550, 814, 17302, 1614, 17903, 8603, 11, 439, 304, 18157, 323, 19278, 11, 719, 1063, 315, 1124, 1101, 20011, 24020, 439, 30617, 315, 279, 56408, 13, 99671, 2617, 13865, 1051, 11953, 704, 2403, 14971, 315, 1614, 11, 1063, 315, 902, 1051, 6992, 13, 1556, 1132, 1705, 1101, 3952, 961, 304, 93574, 13, 9176, 93134, 11, 5423, 279, 10845, 2742, 1705, 11, 11846, 430, 1521, 13865, 1053, 387, 279, 3242, 64476, 369, 264, 14110, 2403, 32682, 323, 279, 1614, 13, 9176, 315, 1521, 8951, 1051, 2884, 555, 3927, 79788, 1821, 323, 279, 8857, 3952, 2035, 304, 279, 3389, 220, 9674, 15, 82, 11, 279, 4216, 220, 9367, 15, 82, 323, 279, 220, 9378, 15, 82, 11, 449, 1063, 2103, 31965, 304, 279, 4216, 220, 7028, 15, 82, 13, 11205, 18979, 304, 38009, 574, 279, 1121, 315, 4726, 31752, 2410, 323, 25103, 323, 16808, 287, 555, 1614, 14673, 8794, 1132, 380, 39555, 7119, 9349, 617, 2744, 1027, 20733, 13, 1556, 277, 971, 2320, 582, 333, 1705, 29115, 369, 2536, 76827, 768, 3445, 311, 11322, 872, 1614, 1752, 11, 2536, 90162, 10548, 13, 7089, 78431, 5315, 29115, 2167, 1957, 11, 264, 39001, 902, 649, 2997, 14385, 315, 80753, 477, 24020, 13, 1115, 19451, 574, 5115, 21102, 264, 9478, 4227, 994, 9298, 279, 1614, 439, 264, 43049, 519, 323, 1063, 93134, 35090, 430, 814, 1047, 1475, 1314, 311, 34134, 1202, 47748, 555, 904, 3445, 3284, 13, 36035, 48428, 323, 15863, 4042, 8560, 14217, 64, 11, 889, 1051, 71796, 315, 7347, 1005, 315, 9349, 11, 11224, 430, 9349, 374, 16632, 264, 13010, 311, 1614, 9349, 439, 264, 5995, 14289, 8794, 1132, 1705, 3952, 459, 4642, 3560, 304, 13471, 6299, 11, 8051, 814, 49890, 311, 387, 3276, 575, 589, 5411, 311, 16287, 22013, 950, 2191, 11, 9298, 433, 439, 15180, 380, 13, 2435, 5602, 433, 439, 264, 961, 315, 279, 7351, 902, 16495, 311, 63331, 279, 1614, 323, 32682, 13, 1556, 1132, 1705, 1101, 49680, 872, 30617, 2949, 279, 19071, 11, 1063, 315, 8884, 44664, 14818, 2191, 323, 42541, 2191, 13, 13266, 93134, 1101, 5918, 10977, 902, 1051, 3196, 389, 27607, 323, 1051, 6532, 304, 279, 3754, 3772, 2887, 65272, 661, 26411, 763, 279, 1510, 11639, 11, 15155, 78431, 42592, 78, 13789, 22278, 11, 264, 463, 1166, 315, 1672, 40304, 661, 44565, 2191, 11, 706, 69767, 660, 279, 11249, 389, 9349, 555, 63686, 279, 2536, 56251, 768, 39001, 18306, 2533, 279, 3389, 220, 777, 339, 9478, 555, 735, 897, 354, 8148, 323, 1023, 21102, 93134, 27905, 13, 11995, 13789, 22278, 323, 279, 8753, 1912, 578, 90138, 10554, 29115, 369, 2678, 11, 42887, 53891, 5315, 11, 1405, 1855, 4562, 374, 8647, 369, 872, 1866, 6299, 719, 4375, 3871, 311, 4546, 1523, 47748, 35988, 80753, 323, 1023, 16806, 3445, 2403, 1614, 11, 32682, 11, 323, 1023, 14207, 13, 17384, 315, 578, 90138, 10554, 1051, 12800, 304, 220, 1049, 23, 389, 5370, 10405, 11, 24020, 5343, 13, 28589, 11, 19225, 93134, 527, 1790, 2753, 16806, 323, 52382, 1109, 872, 42933, 38618, 13, 2435, 10213, 16988, 304, 75867, 279, 4379, 2391, 44895, 323, 61096, 11, 5423, 304, 5961, 1778, 439, 7008, 11, 25431, 11, 323, 12550, 13, 68285, 519, 3776, 41840, 8835, 5315, 527, 3967, 369, 1206, 19587, 449, 279, 4379, 26, 4869, 11, 93134, 539, 1193, 41003, 449, 1614, 20197, 11, 814, 1101, 16988, 304, 279, 14993, 2403, 15550, 1705, 323, 9148, 1705, 11, 4737, 7294, 2269, 5171, 380, 1957, 323, 29905, 4954, 311, 5471, 12491, 57431, 505, 12765, 5253, 65272, 661, 26411, 1556, 1132, 1705, 17037, 3539, 2167, 1957, 13, 1115, 649, 1935, 279, 1376, 315, 87843, 323, 59310, 2403, 52583, 30022, 11, 477, 279, 1376, 315, 659, 21110, 4210, 872, 6439, 1555, 279, 9886, 315, 5663, 3502, 93778, 1778, 439, 1081, 8699, 323, 2536, 2902, 100050, 6667, 1924, 13, 41525, 28846, 374, 3629, 18073, 304, 459, 7294, 43802, 20631, 1648, 11, 449, 5127, 3515, 6273, 2019, 304, 1855, 5597, 11, 459, 5603, 3967, 439, 16600, 2191, 13, 48302, 52443, 93134, 617, 1027, 23387, 449, 5370, 57193, 19567, 430, 527, 810, 477, 2753, 3196, 389, 16600, 2191, 11, 8051, 539, 21650, 78431, 11, 69515, 4443, 51360, 323, 24435, 304, 3148, 55280, 1778, 439, 23170, 323, 44895, 13, 763, 13168, 449, 279, 2466, 6830, 44565, 2191, 315, 279, 29924, 11639, 11, 279, 13945, 78718, 4751, 2678, 7561, 44565, 2191, 17738, 872, 31954, 539, 311, 2385, 872, 11555, 323, 6299, 389, 29924, 52443, 44565, 2191, 477, 311, 8464, 311, 29924, 93134, 1778, 439, 11291, 735, 897, 354, 8148, 323, 38077, 12278, 974, 764, 393, 583, 31721, 263, 311, 9541, 872, 18463, 13, 13266, 93134, 1053, 4856, 2385, 872, 3463, 323, 550, 7332, 389, 872, 1866, 3217, 902, 814, 690, 3010, 46820, 553, 11829, 5597, 28846, 1920, 315, 2678, 78431, 51552, 5315, 11335, 264, 5199, 39747, 3560, 13, 1556, 1132, 1705, 617, 20011, 5370, 5528, 304, 2015, 311, 1977, 264, 11413, 24811, 4315, 3697, 315, 872, 1912, 2085, 279, 1205, 315, 264, 7808, 477, 264, 6522, 1912, 13, 3861, 1648, 374, 369, 459, 3927, 505, 279, 1912, 311, 1514, 279, 3560, 315, 17028, 859, 311, 1520, 11322, 264, 24811, 2085, 4737, 961, 304, 279, 10430, 5694, 477, 22923, 264, 3230, 1486, 13, 30893, 1385, 6118, 4287, 11413, 24811, 11, 3734, 994, 814, 2733, 279, 14050, 23093, 31095, 78431, 32008, 11, 9021, 323, 2819, 13, 1556, 1132, 1705, 6118, 1376, 2678, 5315, 320, 20, 4235, 508, 7931, 8, 311, 18885, 51360, 323, 63081, 4315, 872, 3697, 13, 4314, 13124, 315, 5315, 810, 3629, 1109, 539, 958, 6595, 449, 1855, 1023, 11, 30164, 8294, 14488, 13, 1556, 1132, 1705, 2103, 1862, 323, 16136, 304, 23170, 11, 5423, 8545, 4719, 23170, 439, 1521, 527, 7808, 1752, 23170, 539, 39433, 85413, 555, 264, 22013, 8630, 20855, 304, 279, 3347, 11, 32594, 323, 42780, 527, 1511, 11, 323, 93134, 617, 8208, 2930, 304, 279, 4435, 33845, 5000, 311, 9041, 872, 1984, 13, 1556, 1132, 1705, 617, 1766, 433, 8831, 311, 1893, 13335, 1606, 315, 8141, 278, 323, 1023, 27129, 11, 20256, 14683, 20797, 323, 1023, 76563, 13, 1556, 1132, 1705, 1051, 1101, 6532, 304, 11469, 5370, 3241, 430, 527, 2561, 369, 1949, 13, 578, 1648, 1521, 17524, 105020, 1705, 990, 311, 2274, 323, 16822, 53291, 279, 78431, 52805, 11, 5423, 994, 433, 4131, 311, 47995, 3932, 6, 12625, 505, 1614, 22156, 8794, 1132, 1705, 31335, 5694, 311, 58570, 323, 58583, 586, 12908, 13, 12220, 3062, 4455, 1778, 439, 22670, 323, 994, 12908, 527, 1694, 25366, 11, 814, 527, 3629, 2663, 55548, 97548, 95686, 320, 15559, 57, 705, 12908, 1405, 1989, 11, 32349, 11, 323, 65623, 2191, 527, 55645, 311, 3113, 279, 78431, 10728, 13, 1666, 3970, 555, 93134, 11, 58570, 1303, 374, 264, 1648, 311, 49655, 16036, 3634, 505, 279, 41067, 3157, 11, 13788, 75336, 278, 3966, 323, 1101, 1694, 459, 77381, 2167, 1957, 13, 6515, 71194, 3634, 20682, 93134, 311, 9526, 449, 872, 6848, 323, 1977, 3674, 27460, 13, 31470, 709, 1521, 26411, 1418, 3515, 304, 4059, 430, 539, 682, 93134, 4430, 279, 1890, 33726, 7119, 1124, 11, 3235, 449, 5370, 7739, 315, 59310, 520, 7701, 36396, 4455, 11, 1304, 709, 264, 97733, 3916, 593, 16975, 430, 374, 961, 315, 19225, 78431, 18434, 19025, 9807, 4819, 1666, 44565, 2191, 374, 264, 19675, 430, 95122, 1690, 17226, 33726, 11, 61555, 11, 323, 8853, 315, 3463, 26, 62646, 927, 4860, 315, 2819, 11, 34649, 11, 323, 26411, 374, 4279, 13, 11699, 20057, 706, 6197, 311, 13882, 2204, 5829, 315, 20086, 3878, 4315, 2204, 78431, 32006, 902, 706, 3549, 264, 1396, 315, 36222, 3079, 10742, 304, 78431, 10334, 13, 578, 25780, 315, 32682, 11, 60533, 11, 323, 13901, 449, 44565, 2191, 374, 13882, 55026, 11, 323, 44565, 2191, 32838, 6485, 12135, 449, 89971, 1778, 439, 71189, 11, 6667, 74050, 11, 83815, 11, 323, 6696, 11552, 2191, 13, 1556, 1132, 1705, 1253, 387, 27762, 555, 3823, 2191, 11, 30467, 11447, 11, 82315, 659, 65873, 11, 30249, 2191, 11, 477, 904, 1396, 315, 10778, 31308, 83258, 13, 69883, 6431, 64, 1778, 439, 8431, 8082, 11, 5557, 320, 68, 1326, 13, 2949, 459, 277, 971, 27748, 2408, 74050, 705, 323, 279, 26623, 1920, 1253, 387, 46473, 60479, 2949, 1063, 78431, 61555, 323, 25291, 1208, 38477, 304, 3885, 1246, 1693, 11, 39275, 11, 323, 1949, 3021, 1666, 10026, 323, 39275, 6920, 3235, 1124, 30295, 315, 30022, 11, 1690, 93134, 2686, 11, 49586, 11, 323, 34134, 279, 46735, 315, 832, 596, 51360, 27070, 555, 10026, 13073, 815, 327, 10965, 574, 539, 3629, 14407, 555, 29924, 93134, 719, 279, 2478, 430, 1550, 6612, 430, 459, 78431, 8396, 1053, 3063, 311, 39275, 18182, 11469, 13, 39767, 9349, 574, 264, 4747, 369, 93134, 1778, 439, 30411, 56256, 11, 889, 16475, 4325, 315, 14771, 7016, 11, 35090, 814, 1053, 8935, 88170, 3026, 13, 362, 13970, 1510, 430, 51063, 323, 20415, 3384, 2391, 220, 9378, 15, 323, 220, 5926, 15, 2949, 44565, 2191, 574, 1949, 3021, 13, 763, 19225, 44565, 2191, 11, 420, 1510, 83417, 439, 264, 31954, 311, 1862, 10062, 309, 683, 323, 55641, 44565, 2191, 13, 3658, 3021, 28424, 1051, 2403, 11103, 11, 902, 814, 5602, 439, 264, 1648, 315, 3026, 49941, 11447, 927, 3278, 11, 14090, 1606, 11103, 2383, 19407, 92867, 279, 2410, 315, 3026, 13, 578, 23035, 315, 1949, 3021, 574, 1790, 27927, 323, 5343, 264, 43665, 315, 279, 9749, 2015, 430, 7347, 3278, 596, 7392, 11542, 323, 17069, 13, 13266, 1949, 3021, 19567, 20162, 311, 279, 21967, 315, 57937, 15316, 11, 1405, 3544, 5315, 315, 40386, 11, 93134, 323, 1023, 21572, 46498, 304, 28036, 3871, 13, 3658, 3021, 1047, 20282, 2225, 304, 4606, 323, 279, 3723, 4273, 26, 4869, 11, 1063, 93134, 28214, 449, 279, 80822, 430, 51063, 505, 1949, 3021, 13, 1556, 1132, 380, 65865, 1051, 28424, 315, 1949, 3021, 11, 2403, 11103, 11, 323, 463, 63726, 320, 1338, 3876, 264, 19225, 4751, 705, 323, 1047, 264, 4528, 18909, 13, 1556, 1132, 380, 323, 2536, 19415, 1132, 380, 65865, 89075, 389, 8657, 48468, 719, 1051, 33445, 315, 832, 2500, 920, 1711, 279, 2132, 4376, 315, 279, 220, 508, 339, 9478, 11, 44565, 2191, 958, 5424, 839, 449, 279, 2132, 12330, 315, 53110, 11, 18336, 3876, 1063, 60701, 315, 279, 38012, 7351, 323, 1694, 28160, 439, 1664, 13, 3296, 279, 5652, 11026, 315, 279, 220, 508, 339, 9478, 11, 93134, 323, 65865, 1051, 59416, 369, 279, 3268, 323, 51360, 315, 3278, 11, 57577, 11, 1744, 388, 323, 1023, 32873, 4147, 5315, 11, 449, 1063, 38012, 69122, 23377, 264, 37608, 315, 279, 1403, 60701, 13, 3161, 279, 4948, 12330, 315, 53110, 11, 7392, 9764, 323, 65868, 30548, 20377, 10965, 6244, 264, 3917, 315, 4007, 369, 93134, 11, 78504, 264, 1772, 12, 96797, 380, 43665, 315, 7392, 4725, 488, 13, 4427, 93134, 1612, 4979, 5694, 505, 420, 1584, 315, 7422, 11, 23377, 430, 433, 55939, 7119, 459, 3927, 2191, 430, 574, 26156, 279, 5353, 315, 3674, 55633, 8794, 1132, 2191, 323, 6873, 578, 2802, 315, 93134, 304, 6873, 50699, 1203, 311, 279, 1176, 49179, 315, 29924, 44565, 2191, 13, 1556, 1132, 1705, 2980, 6300, 6873, 11, 832, 902, 7437, 279, 41582, 315, 279, 3938, 51360, 315, 279, 3927, 323, 279, 8396, 11, 311, 387, 459, 1180, 315, 27848, 12576, 13, 1556, 1132, 380, 16483, 1778, 439, 12656, 4359, 7678, 320, 81147, 12007, 8, 323, 7639, 65292, 1215, 3573, 791, 3641, 82086, 315, 5751, 11930, 909, 18855, 2225, 1614, 6873, 323, 879, 6873, 439, 2500, 3445, 555, 902, 279, 17864, 538, 46113, 872, 36832, 5450, 220, 7028, 16, 11, 81944, 78431, 323, 1949, 98636, 13175, 29042, 261, 9749, 279, 15917, 24669, 18766, 64, 304, 28035, 439, 459, 14076, 311, 279, 9749, 6873, 1887, 902, 574, 81498, 14090, 555, 279, 16879, 9441, 13, 29042, 261, 596, 5603, 574, 37019, 11, 63686, 2225, 1614, 323, 8993, 22315, 304, 279, 16627, 1920, 24797, 7231, 45172, 3544, 15055, 315, 51360, 304, 9293, 872, 990, 323, 28116, 13, 29042, 261, 20034, 311, 39170, 279, 3318, 538, 323, 21650, 16495, 311, 31087, 538, 25917, 4315, 4236, 13, 578, 2978, 8036, 1306, 6926, 26425, 555, 279, 1614, 323, 29042, 261, 574, 3010, 12800, 13, 56733, 11, 813, 6848, 14454, 279, 20343, 369, 264, 4101, 315, 6617, 8853, 2212, 279, 1917, 13, 9052, 78431, 37848, 59602, 267, 2303, 11, 889, 4756, 279, 9071, 11930, 323, 21023, 11, 1101, 9749, 264, 4528, 2978, 449, 1202, 36330, 17966, 1694, 430, 330, 2000, 6873, 311, 387, 7524, 433, 1047, 311, 387, 1949, 1210, 763, 264, 4528, 4037, 11, 362, 13, 328, 13, 4275, 484, 18538, 1148, 6244, 279, 19367, 28607, 6150, 304, 220, 5926, 16, 11, 1101, 37631, 1694, 1949, 505, 78242, 8794, 1132, 380, 6873, 374, 3196, 14090, 389, 279, 4623, 430, 264, 1716, 596, 1314, 311, 2274, 26662, 323, 2085, 34786, 22525, 311, 387, 31387, 323, 430, 25442, 488, 1053, 3063, 2911, 311, 57323, 1695, 31342, 26, 4869, 11, 1070, 706, 1027, 2697, 24811, 4315, 78431, 12678, 439, 311, 1148, 42675, 34786, 13, 29042, 261, 11846, 430, 16033, 42100, 10820, 2617, 574, 5995, 323, 21650, 15972, 45172, 430, 22526, 11, 31220, 323, 3674, 12437, 1051, 539, 3284, 1234, 32682, 11, 3235, 449, 1023, 87313, 315, 3109, 323, 60533, 1236, 349, 220, 508, 339, 9478, 323, 19225, 78431, 16483, 320, 26368, 71492, 11, 58463, 4557, 11, 323, 40979, 27738, 8, 79849, 323, 17626, 279, 78431, 43665, 315, 1614, 6873, 11, 14090, 21760, 389, 279, 1205, 369, 264, 1887, 430, 24400, 389, 2911, 596, 28697, 4856, 1109, 389, 872, 5845, 311, 36861, 264, 7076, 477, 16136, 304, 11761, 2191, 439, 961, 315, 264, 11761, 8396, 13, 48302, 93134, 1778, 439, 27738, 3802, 430, 1614, 6873, 17482, 311, 22313, 6426, 80431, 32305, 95386, 2478, 78431, 6873, 14673, 617, 26968, 311, 279, 6617, 11477, 11, 3682, 5899, 1441, 315, 78431, 8853, 11, 4315, 1124, 5201, 369, 1716, 51360, 323, 39661, 389, 33811, 4856, 1109, 42100, 10820, 2617, 439, 264, 12917, 1749, 11, 617, 9041, 4315, 21391, 16627, 14673, 13, 79193, 17360, 22144, 5144, 2380, 8853, 439, 21650, 93134, 8853, 11, 32125, 279, 3658, 4923, 1786, 16376, 21510, 304, 279, 3723, 4273, 902, 374, 961, 315, 264, 22622, 3778, 12, 60674, 4009, 315, 8853, 11, 279, 10323, 12, 28291, 21579, 9304, 304, 58519, 11, 9635, 11, 323, 279, 16056, 579, 689, 6150, 304, 18157, 8794, 1132, 2191, 323, 279, 1614, 4761, 7761, 311, 279, 1614, 323, 1202, 14673, 374, 264, 58768, 74625, 2536, 315, 44565, 2191, 13, 1556, 1132, 1705, 2980, 279, 1614, 439, 264, 5507, 315, 55949, 323, 4510, 433, 311, 387, 30067, 71850, 15851, 315, 1202, 5054, 61555, 13, 12361, 315, 1274, 1694, 3025, 311, 2585, 279, 13878, 315, 872, 2324, 11, 3682, 11429, 527, 4529, 555, 264, 2678, 21342, 13, 22677, 13967, 54331, 21742, 389, 2410, 11, 15851, 315, 3508, 430, 2410, 374, 1825, 477, 18300, 11, 439, 433, 2103, 706, 279, 5845, 311, 84125, 1274, 13, 13596, 78431, 5811, 2403, 5415, 374, 430, 279, 1274, 9129, 10831, 264, 3109, 11, 1524, 279, 1455, 93592, 4633, 4315, 7510, 11, 690, 78302, 2915, 6056, 311, 8895, 810, 2410, 11, 6522, 311, 21948, 13, 1556, 1132, 1705, 2980, 279, 4623, 430, 279, 1614, 374, 279, 22498, 690, 315, 279, 1274, 311, 387, 459, 653, 45620, 24694, 17422, 4245, 311, 279, 2144, 430, 279, 17864, 538, 374, 12742, 505, 279, 2800, 315, 8396, 815, 15934, 78431, 33726, 7119, 279, 1614, 13592, 13, 8563, 7043, 96245, 11846, 430, 279, 24408, 1990, 11447, 323, 51360, 1053, 3152, 279, 1614, 1436, 2646, 387, 23583, 13, 36769, 359, 258, 5602, 279, 1614, 439, 7438, 330, 1030, 3035, 290, 11, 55949, 555, 3445, 315, 78242, 11, 6730, 62840, 3359, 422, 3284, 719, 653, 3913, 7439, 1245, 323, 43661, 422, 1205, 387, 1210, 362, 13, 3842, 51196, 323, 54783, 7997, 11, 889, 55939, 9017, 41903, 44565, 2191, 11, 11846, 430, 279, 1614, 1436, 387, 23583, 422, 433, 374, 27800, 555, 24811, 11, 8051, 814, 5602, 420, 439, 7701, 17821, 13, 7984, 30921, 389, 1268, 311, 90376, 279, 1614, 1101, 1782, 8794, 1132, 2191, 323, 279, 19071, 578, 3717, 1990, 44565, 2191, 323, 1989, 574, 5115, 28254, 2391, 279, 29924, 11639, 315, 44565, 2191, 11, 5423, 4315, 32692, 60701, 430, 1051, 11469, 2391, 430, 11639, 1778, 439, 18111, 324, 1705, 11, 65623, 1705, 323, 3885, 13, 763, 17649, 11, 44565, 2191, 574, 10213, 5938, 449, 279, 1561, 5345, 67780, 418, 1233, 323, 279, 36182, 12, 442, 8322, 2191, 7351, 13, 763, 4731, 11, 44565, 2191, 706, 1027, 5938, 449, 4731, 16451, 1778, 439, 36858, 13, 1556, 1132, 1705, 1778, 439, 37848, 59602, 267, 2303, 323, 58463, 4557, 11224, 430, 279, 3973, 1990, 279, 10255, 323, 279, 2536, 12, 19135, 11, 1148, 62849, 1989, 505, 264, 7446, 1180, 11, 374, 264, 9429, 9124, 555, 279, 20167, 367, 9057, 555, 32682, 323, 433, 29034, 12966, 505, 5496, 264, 83212, 2324, 96442, 93134, 64854, 369, 477, 1511, 1989, 439, 264, 3445, 311, 11322, 78431, 10548, 13, 763, 813, 2363, 52624, 279, 30180, 25, 362, 11346, 315, 1556, 1132, 380, 16807, 3906, 8476, 11, 82601, 354, 2070, 80883, 34344, 11, 323, 14434, 27825, 30826, 11, 11517, 4997, 978, 8349, 430, 330, 276, 1132, 380, 48336, 2258, 12659, 617, 15098, 34030, 7351, 6108, 2835, 55280, 1210, 46982, 279, 220, 508, 339, 9478, 11, 1690, 21102, 93134, 320, 37659, 735, 897, 354, 8148, 11, 36035, 48428, 11, 49720, 402, 11680, 28196, 323, 8215, 22532, 14502, 31803, 8, 323, 29085, 1778, 439, 1556, 15630, 6267, 922, 13146, 47031, 311, 279, 19071, 5221, 770, 50917, 6012, 1903, 1989, 5505, 311, 93134, 13, 1102, 1436, 43504, 264, 43665, 315, 6484, 8396, 323, 12694, 1132, 552, 11, 8854, 439, 264, 864, 127848, 1413, 5507, 311, 8881, 279, 78431, 10728, 8396, 323, 1524, 2543, 1139, 264, 3445, 315, 2167, 1957, 1778, 439, 304, 22670, 13, 1666, 433, 35730, 311, 2225, 20356, 323, 2944, 11, 1989, 1436, 14638, 311, 279, 4459, 3823, 323, 617, 264, 8147, 2515, 13, 578, 220, 777, 339, 34457, 36182, 38025, 4099, 380, 7351, 1047, 459, 50953, 37637, 323, 9076, 459, 3187, 315, 459, 78431, 21063, 315, 279, 5754, 7119, 51618, 13, 763, 11876, 523, 460, 625, 4918, 264, 15796, 3919, 555, 78431, 30581, 8215, 4618, 393, 1056, 82301, 11, 279, 56941, 315, 37637, 323, 3674, 26348, 374, 864, 915, 1711, 459, 10728, 44565, 4633, 40574, 8997, 4029, 8794, 9270, 578, 1455, 4279, 43665, 315, 44565, 2191, 374, 430, 12966, 4250, 659, 2427, 6846, 323, 779, 264, 1614, 374, 5995, 369, 3823, 20237, 13, 7302, 46459, 9084, 95674, 25953, 7396, 420, 43665, 11, 28898, 430, 10768, 79, 60, 68, 580, 323, 4208, 11, 44995, 11, 14640, 315, 95629, 4787, 323, 279, 6412, 315, 912, 30351, 11217, 11, 279, 46643, 315, 264, 1120, 1887, 315, 8141, 25, 1521, 11, 4315, 3885, 11, 527, 5865, 902, 1436, 20781, 387, 10887, 304, 264, 4029, 304, 902, 1070, 574, 912, 8792, 3109, 1210, 13596, 4279, 19347, 315, 44565, 2191, 374, 430, 433, 18809, 264, 1917, 315, 31398, 304, 902, 1193, 279, 2678, 3403, 15086, 649, 387, 659, 2427, 2017, 1251, 26, 264, 2077, 1053, 387, 430, 3682, 78431, 69122, 64854, 78431, 6918, 2191, 37089, 321, 11597, 88, 72235, 13929, 480, 13, 435, 532, 64, 24306, 264, 1160, 315, 4279, 6105, 2403, 44565, 2191, 902, 5764, 87313, 1778, 439, 430, 44565, 2191, 374, 6301, 2718, 5552, 311, 9349, 323, 19814, 11, 539, 1193, 304, 279, 75336, 1917, 11, 1778, 439, 520, 22670, 11, 719, 304, 279, 1917, 315, 32008, 439, 1664, 13, 72131, 11, 44565, 2191, 374, 26126, 439, 653, 90377, 1260, 477, 8791, 48748, 2533, 279, 1614, 4250, 387, 24164, 32367, 13, 1115, 1584, 315, 6105, 1455, 3629, 6880, 369, 5054, 1957, 2949, 279, 1887, 311, 15180, 433, 13, 578, 4948, 5811, 374, 430, 44565, 2191, 374, 659, 12, 8386, 329, 91133, 13, 6104, 433, 28424, 369, 912, 19101, 311, 5438, 77099, 11, 422, 11928, 555, 279, 1690, 11, 1243, 44565, 2191, 1053, 2543, 1139, 279, 17864, 5054, 10334, 13, 763, 420, 1584, 315, 19347, 1101, 4131, 279, 659, 12, 8386, 329, 2538, 430, 44565, 2191, 6880, 369, 22498, 1957, 24797, 98859, 279, 51360, 315, 279, 3927, 11, 16472, 912, 22498, 1957, 649, 387, 4529, 13, 71809, 11, 435, 532, 64, 34945, 264, 43665, 7119, 41903, 44565, 2191, 315, 1694, 55288, 320, 543, 3137, 323, 11555, 8, 323, 304, 279, 33953, 32682, 323, 53416, 538, 8625, 3831, 37089, 321, 11597, 950, 44565, 2191, 706, 2322, 279, 19347, 315, 3697, 315, 67671, 2768, 279, 4984, 315, 463, 19415, 1132, 380, 6603, 1778, 439, 362, 13, 3842, 51196, 6, 89606, 58014, 323, 31597, 56875, 343, 811, 13, 7658, 14561, 12656, 362, 13, 71911, 942, 67213, 459, 9071, 311, 18046, 2403, 2380, 3682, 41903, 78431, 16565, 902, 568, 14035, 4498, 19995, 13, 71911, 942, 2795, 430, 1418, 279, 3927, 1587, 539, 42210, 279, 1614, 264, 14523, 315, 67194, 11, 420, 1587, 539, 34608, 430, 44565, 2191, 374, 279, 31352, 17102, 323, 279, 1614, 374, 2103, 57323, 23583, 13, 763, 578, 22854, 315, 31597, 22677, 11, 8096, 22546, 41996, 82060, 41903, 44565, 2191, 11, 21039, 430, 330, 75785, 11447, 374, 264, 16033, 41919, 1210, 4054, 315, 279, 30758, 63836, 374, 430, 44565, 2191, 711, 552, 323, 14865, 311, 3619, 279, 24156, 77004, 311, 11447, 13, 15466, 77919, 5415, 430, 279, 26586, 315, 11447, 24897, 279, 16801, 430, 2768, 872, 11470, 690, 10150, 810, 2450, 13, 77919, 13919, 430, 420, 5811, 374, 837, 304, 2768, 2225, 11527, 6, 6992, 323, 37104, 7754, 13, 1556, 1132, 1705, 8007, 420, 19347, 1606, 17436, 477, 68735, 1216, 287, 11447, 1587, 539, 87092, 279, 52979, 315, 1202, 22934, 555, 61708, 11447, 1778, 439, 16410, 477, 21866, 439, 15062, 11, 6463, 1587, 433, 21736, 264, 4686, 32859, 315, 9678, 19971, 13, 1556, 1132, 380, 21063, 315, 3823, 7138, 11, 38001, 315, 279, 1614, 11, 323, 15507, 311, 3674, 14110, 706, 1027, 60479, 555, 48709, 439, 50765, 11, 39532, 78630, 11, 323, 71985, 11, 15947, 13, 68533, 44565, 2191, 706, 1027, 60479, 369, 39661, 2288, 17345, 389, 279, 16801, 430, 279, 76445, 315, 279, 1614, 690, 3063, 311, 3823, 23915, 29761, 287, 1006, 4588, 14172, 3365, 2053, 11, 6646, 311, 387, 832, 315, 279, 12717, 48727, 315, 83815, 11, 60479, 44565, 2191, 596, 7294, 43802, 20631, 2191, 439, 49188, 5663, 5621, 65272, 661, 1606, 304, 813, 1684, 264, 14110, 374, 555, 5196, 59021, 13, 42170, 3842, 386, 5849, 818, 2249, 14238, 304, 813, 2363, 1556, 1132, 2191, 25, 362, 70050, 34307, 42914, 430, 330, 276, 1132, 2191, 4250, 3243, 498, 35090, 430, 433, 37856, 279, 5845, 311, 10489, 4305, 1202, 6848, 13, 578, 70050, 19347, 315, 44565, 2191, 374, 430, 433, 706, 264, 8791, 48748, 3752, 1606, 682, 7931, 1288, 617, 78431, 6325, 323, 2819, 13, 10771, 311, 279, 70050, 1684, 11, 430, 264, 3674, 4623, 1053, 1833, 6089, 505, 420, 3823, 10728, 323, 704, 315, 279, 1949, 690, 315, 1475, 3927, 14454, 1202, 28591, 13, 28187, 1705, 1614, 430, 420, 50859, 574, 8647, 369, 872, 38550, 311, 1180, 13, 763, 279, 78431, 11376, 11, 279, 12324, 1990, 31220, 323, 22526, 574, 20250, 1555, 1080, 93772, 323, 81050, 5859, 815, 2176, 1101, 220, 1556, 1132, 2191, 555, 3224, 81689, 2085, 3109, 1796, 315, 78431, 5054, 89971, 1796, 315, 6603, 922, 44565, 2191, 32812, 34, 31767, 22405, 33300, 16352, 8336, 50063, 8336, 51, 67630, 8336, 31428, 5403, 262, 34307, 42914, 315, 41903, 44565, 2191, 13, 256, 362, 23682, 315, 41903, 44565, 2191, 11, 28898, 430, 330, 21704, 13124, 315, 364, 276, 1132, 2191, 6, 510, 72, 1770, 13, 41903, 323, 5054, 44565, 2191, 60, 527, 41903, 323, 5054, 8349, 1210, 320, 79, 13, 4194, 10148, 8, 220, 1556, 1132, 4633, 5526, 17422, 11775, 13, 257, 1556, 5811, 369, 41903, 44565, 2191, 5231, 15702, 7902, 220, 1556, 15630]
inputs:
Anarchism is a political philosophy and movement that is sceptical of authority and rejects all involuntary, coercive forms of hierarchy. Anarchism calls for the abolition of the state, which it holds to be unnecessary, undesirable, and harmful. As a historically left-wing movement, placed on the farthest left of the political spectrum, it is usually described alongside communalism and libertarian Marxism as the libertarian wing (libertarian socialism) of the socialist movement, and has a strong historical association with anti-capitalism and socialism.Humans lived in societies without formal hierarchies long before the establishment of formal states, realms, or empires. With the rise of organised hierarchical bodies, scepticism toward authority also rose. Although traces of anarchist thought are found throughout history, modern anarchism emerged from the Enlightenment. During the latter half of the 19th and the first decades of the 20th century, the anarchist movement flourished in most parts of the world and had a significant role in workers' struggles for emancipation. Various anarchist schools of thought formed during this period. Anarchists have taken part in several revolutions, most notably in the Paris Commune, the Russian Civil War and the Spanish Civil War, whose end marked the end of the classical era of anarchism. In the last decades of the 20th and into the 21st century, the anarchist movement has been resurgent once more.Anarchism employs a diversity of tactics in order to meet its ideal ends which can be broadly separated into revolutionary and evolutionary tactics; there is significant overlap between the two, which are merely descriptive. Revolutionary tactics aim to bring down authority and state, having taken a violent turn in the past, while evolutionary tactics aim to prefigure what an anarchist society would be like. Anarchist thought, criticism, and praxis have played a part in diverse areas of human society. Criticism of anarchism include claims that it is internally inconsistent, violent, or utopian.Etymology, terminology, and definition The etymological origin of anarchism is from the Ancient Greek anarkhia, meaning "without a ruler", composed of the prefix an- ("without") and the word arkhos ("leader" or "ruler"). The suffix -ism denotes the ideological current that favours anarchy. Anarchism appears in English from 1642 as anarchisme and anarchy from 1539; early English usages emphasised a sense of disorder. Various factions within the French Revolution labelled their opponents as anarchists, although few such accused shared many views with later anarchists. Many revolutionaries of the 19th century such as William Godwin (1756–1836) and Wilhelm Weitling (1808–1871) would contribute to the anarchist doctrines of the next generation but did not use anarchist or anarchism in describing themselves or their beliefs.The first political philosopher to call himself an anarchist () was Pierre-Joseph Proudhon (1809–1865), marking the formal birth of anarchism in the mid-19th century. Since the 1890s and beginning in France, libertarianism has often been used as a synonym for anarchism and its use as a synonym is still common outside the United States. Some usages of libertarianism refer to individualistic free-market philosophy only, and free-market anarchism in particular is termed libertarian anarchism.While the term libertarian has been largely synonymous with anarchism, its meaning has more recently diluted with wider adoption from ideologically disparate groups, including both the New Left and libertarian Marxists, who do not associate themselves with authoritarian socialists or a vanguard party, and extreme cultural liberals, who are primarily concerned with civil liberties. Additionally, some anarchists use libertarian socialist to avoid anarchism's negative connotations and emphasise its connections with socialism. Anarchism is broadly used to describe the anti-authoritarian wing of the socialist movement. Anarchism is contrasted to socialist forms which are state-oriented or from above. Scholars of anarchism generally highlight anarchism's socialist credentials and criticise attempts at creating dichotomies between the two. Some scholars describe anarchism as having many influences from liberalism, and being both liberals and socialists but more so, while most scholars reject anarcho-capitalism as a misunderstanding of anarchist principles.While opposition to the state is central to anarchist thought, defining anarchism is not an easy task for scholars, as there is a lot of discussion among scholars and anarchists on the matter, and various currents perceive anarchism slightly differently. Major definitional elements include the will for a non-coercive society, the rejection of the state apparatus, the belief that human nature allows humans to exist in or progress toward such a non-coercive society, and a suggestion on how to act to pursue the ideal of anarchy.HistoryPre-modern era Before the establishment of towns and cities, an established authority did not exist. It was after the creation of institutions of authority that anarchistic ideas espoused as a reaction. The most notable precursors to anarchism in the ancient world were in China and Greece. In China, philosophical anarchism (the discussion on the legitimacy of the state) was delineated by Taoist philosophers Zhuang Zhou and Laozi. Alongside Stoicism, Taoism has been said to have had "significant anticipations" of anarchism. Anarchic attitudes were also articulated by tragedians and philosophers in Greece. Aeschylus and Sophocles used the myth of Antigone to illustrate the conflict between rules set by the state and personal autonomy. Socrates questioned Athenian authorities constantly and insisted on the right of individual freedom of conscience. Cynics dismissed human law (nomos) and associated authorities while trying to live according to nature (physis). Stoics were supportive of a society based on unofficial and friendly relations among its citizens without the presence of a state.In medieval Europe, there was no anarchistic activity except some ascetic religious movements. These, and other Muslim movements, later gave birth to religious anarchism. In the Sasanian Empire, Mazdak called for an egalitarian society and the abolition of monarchy, only to be soon executed by Emperor Kavad I.In Basra, religious sects preached against the state. In Europe, various sects developed anti-state and libertarian tendencies. Renewed interest in antiquity during the Renaissance and in private judgment during the Reformation restored elements of anti-authoritarian secularism, particularly in France. Enlightenment challenges to intellectual authority (secular and religious) and the revolutions of the 1790s and 1848 all spurred the ideological development of what became the era of classical anarchism.Modern era During the French Revolution, partisan groups such as the Enragés and the  saw a turning point in the fermentation of anti-state and federalist sentiments. The first anarchist currents developed throughout the 18th century as William Godwin espoused philosophical anarchism in England, morally delegitimising the state, Max Stirner's thinking paved the way to individualism and Pierre-Joseph Proudhon's theory of mutualism found fertile soil in France. By the late 1870s, various anarchist schools of thought had become well-defined and a wave of then unprecedented globalisation occurred from 1880 to 1914. This era of classical anarchism lasted until the end of the Spanish Civil War and is considered the golden age of anarchism.Drawing from mutualism, Mikhail Bakunin founded collectivist anarchism and entered the International Workingmen's Association, a class worker union later known as the First International that formed in 1864 to unite diverse revolutionary currents. The International became a significant political force, with Karl Marx being a leading figure and a member of its General Council. Bakunin's faction (the Jura Federation) and Proudhon's followers (the mutualists) opposed state socialism, advocating political abstentionism and small property holdings. After bitter disputes, the Bakuninists were expelled from the International by the Marxists at the 1872 Hague Congress. Anarchists were treated similarly in the Second International, being ultimately expelled in 1896. Bakunin famously predicted that if revolutionaries gained power by Marx's terms, they would end up the new tyrants of workers. In response to their expulsion from the First International, anarchists formed the St. Imier International. Under the influence of Peter Kropotkin, a Russian philosopher and scientist, anarcho-communism overlapped with collectivism. Anarcho-communists, who drew inspiration from the 1871 Paris Commune, advocated for free federation and for the distribution of goods according to one's needs.At the turn of the century, anarchism had spread all over the world. It was a notable feature of the international syndicalism movement. In China, small groups of students imported the humanistic pro-science version of anarcho-communism. Tokyo was a hotspot for rebellious youth from countries of the far east, travelling to the Japanese capital to study. In Latin America, Argentina was a stronghold for anarcho-syndicalism, where it became the most prominent left-wing ideology. During this time, a minority of anarchists adopted tactics of revolutionary political violence. This strategy became known as propaganda of the deed. The dismemberment of the French socialist movement into many groups and the execution and exile of many Communards to penal colonies following the suppression of the Paris Commune favoured individualist political expression and acts. Even though many anarchists distanced themselves from these terrorist acts, infamy came upon the movement and attempts were made to exclude them from American immigration, including the Immigration Act of 1903, also called the Anarchist Exclusion Act. Illegalism was another strategy which some anarchists adopted during this period.Despite concerns, anarchists enthusiastically participated in the Russian Revolution in opposition to the White movement; however, they met harsh suppression after the Bolshevik government was stabilised. Several anarchists from Petrograd and Moscow fled to Ukraine, notably leading to the Kronstadt rebellion and Nestor Makhno's struggle in the Free Territory. With the anarchists being crushed in Russia, two new antithetical currents emerged, namely platformism and synthesis anarchism. The former sought to create a coherent group that would push for revolution while the latter were against anything that would resemble a political party. Seeing the victories of the Bolsheviks in the October Revolution and the resulting Russian Civil War, many workers and activists turned to communist parties which grew at the expense of anarchism and other socialist movements. In France and the United States, members of major syndicalist movements such as the General Confederation of Labour and the Industrial Workers of the World left their organisations and joined the Communist International.In the Spanish Civil War of 1936, anarchists and syndicalists (CNT and FAI) once again allied themselves with various currents of leftists. A long tradition of Spanish anarchism led to anarchists playing a pivotal role in the war. In response to the army rebellion, an anarchist-inspired movement of peasants and workers, supported by armed militias, took control of Barcelona and of large areas of rural Spain, where they collectivised the land. The Soviet Union provided some limited assistance at the beginning of the war, but the result was a bitter fight among communists and anarchists at a series of events named May Days as Joseph Stalin tried to seize control of the Republicans.Post-war era At the end of World War II, the anarchist movement was severely weakened. The 1960s witnessed a revival of anarchism, likely caused by a perceived failure of Marxism–Leninism and tensions built by the Cold War. During this time, anarchism found a presence in other movements critical towards both capitalism and the state such as the anti-nuclear, environmental, and peace movements, the counterculture of the 1960s, and the New Left. It also saw a transition from its previous revolutionary nature to provocative anti-capitalist reformism. Anarchism became associated with punk subculture as exemplified by bands such as Crass and the Sex Pistols. The established feminist tendencies of anarcha-feminism returned with vigour during the second wave of feminism. Black anarchism began to take form at this time and influenced anarchism's move from a Eurocentric demographic. This coincided with its failure to gain traction in Northern Europe and its unprecedented height in Latin America.Around the turn of the 21st century, anarchism grew in popularity and influence within anti-capitalist, anti-war and anti-globalisation movements. Anarchists became known for their involvement in protests against the World Trade Organization (WTO), the Group of Eight and the World Economic Forum. During the protests, ad hoc leaderless anonymous cadres known as black blocs engaged in rioting, property destruction and violent confrontations with the police. Other organisational tactics pioneered in this time include affinity groups, security culture and the use of decentralised technologies such as the Internet. A significant event of this period was the confrontations at the 1999 Seattle WTO conference. Anarchist ideas have been influential in the development of the Zapatistas in Mexico and the Democratic Federation of Northern Syria, more commonly known as Rojava, a de facto autonomous region in northern Syria.Thought Anarchist schools of thought have been generally grouped into two main historical traditions, social anarchism and individualist anarchism, owing to their different origins, values and evolution. The individualist current emphasises negative liberty in opposing restraints upon the free individual, while the social current emphasises positive liberty in aiming to achieve the free potential of society through equality and social ownership. In a chronological sense, anarchism can be segmented by the classical currents of the late 19th century and the post-classical currents (anarcha-feminism, green anarchism, and post-anarchism) developed thereafter.Beyond the specific factions of anarchist movements which constitute political anarchism lies philosophical anarchism which holds that the state lacks moral legitimacy, without necessarily accepting the imperative of revolution to eliminate it. A component especially of individualist anarchism, philosophical anarchism may tolerate the existence of a minimal state but claims that citizens have no moral obligation to obey government when it conflicts with individual autonomy. Anarchism pays significant attention to moral arguments since ethics have a central role in anarchist philosophy. Anarchism's emphasis on anti-capitalism, egalitarianism, and for the extension of community and individuality sets it apart from anarcho-capitalism and other types of economic libertarianism.Anarchism is usually placed on the far-left of the political spectrum. Much of its economics and legal philosophy reflect anti-authoritarian, anti-statist, libertarian, and radical interpretations of left-wing and socialist politics such as collectivism, communism, individualism, mutualism, and syndicalism, among other libertarian socialist economic theories. As anarchism does not offer a fixed body of doctrine from a single particular worldview, many anarchist types and traditions exist and varieties of anarchy diverge widely. One reaction against sectarianism within the anarchist milieu was anarchism without adjectives, a call for toleration and unity among anarchists first adopted by Fernando Tarrida del Mármol in 1889 in response to the bitter debates of anarchist theory at the time. Belief in political nihilism has been espoused by anarchists. Despite separation, the various anarchist schools of thought are not seen as distinct entities but rather as tendencies that intermingle and are connected through a set of uniform principles such as individual and local autonomy, mutual aid, network organisation, communal democracy, justified authority and decentralisation.Classical Inceptive currents among classical anarchist currents were mutualism and individualism. They were followed by the major currents of social anarchism (collectivist, communist and syndicalist). They differ on organisational and economic aspects of their ideal society.Mutualism is an 18th-century economic theory that was developed into anarchist theory by Pierre-Joseph Proudhon. Its aims include reciprocity, free association, voluntary contract, federation and monetary reform of both credit and currency that would be regulated by a bank of the people. Mutualism has been retrospectively characterised as ideologically situated between individualist and collectivist forms of anarchism. In What Is Property? (1840), Proudhon first characterised his goal as a "third form of society, the synthesis of communism and property." Collectivist anarchism is a revolutionary socialist form of anarchism commonly associated with Mikhail Bakunin. Collectivist anarchists advocate collective ownership of the means of production which is theorised to be achieved through violent revolution and that workers be paid according to time worked, rather than goods being distributed according to need as in communism. Collectivist anarchism arose alongside Marxism but rejected the dictatorship of the proletariat despite the stated Marxist goal of a collectivist stateless society.Anarcho-communism is a theory of anarchism that advocates a communist society with common ownership of the means of production, direct democracy and a horizontal network of voluntary associations, workers' councils and worker cooperatives, with production and consumption based on the guiding principle "From each according to his ability, to each according to his need." Anarcho-communism developed from radical socialist currents after the French Revolution but was first formulated as such in the Italian section of the First International. It was later expanded upon in the theoretical work of Peter Kropotkin, whose specific style would go onto become the dominating view of anarchists by the late 19th century. Anarcho-syndicalism is a branch of anarchism that views labour syndicates as a potential force for revolutionary social change, replacing capitalism and the state with a new society democratically self-managed by workers. The basic principles of anarcho-syndicalism are direct action, workers' solidarity and workers' self-management.Individualist anarchism is a set of several traditions of thought within the anarchist movement that emphasise the individual and their will over any kinds of external determinants. Early influences on individualist forms of anarchism include William Godwin, Max Stirner, and Henry David Thoreau. Through many countries, individualist anarchism attracted a small yet diverse following of Bohemian artists and intellectuals as well as young anarchist outlaws in what became known as illegalism and individual reclamation.Post-classical and contemporary Anarchist principles undergird contemporary radical social movements of the left. Interest in the anarchist movement developed alongside momentum in the anti-globalisation movement, whose leading activist networks were anarchist in orientation. As the movement shaped 21st century radicalism, wider embrace of anarchist principles signaled a revival of interest. Anarchism has continued to generate many philosophies and movements, at times eclectic, drawing upon various sources and combining disparate concepts to create new philosophical approaches. The anti-capitalist tradition of classical anarchism has remained prominent within contemporary currents.Contemporary news coverage which emphasizes black bloc demonstrations has reinforced anarchism's historical association with chaos and violence. Its publicity has also led more scholars in fields such as anthropology and history to engage with the anarchist movement, although contemporary anarchism favours actions over academic theory. Various anarchist groups, tendencies, and schools of thought exist today, making it difficult to describe the contemporary anarchist movement. While theorists and activists have established "relatively stable constellations of anarchist principles", there is no consensus on which principles are core and commentators describe multiple anarchisms, rather than a singular anarchism, in which common principles are shared between schools of anarchism while each group prioritizes those principles differently. Gender equality can be a common principle, although it ranks as a higher priority to anarcha-feminists than anarcho-communists.Anarchists are generally committed against coercive authority in all forms, namely "all centralized and hierarchical forms of government (e.g., monarchy, representative democracy, state socialism, etc.), economic class systems (e.g., capitalism, Bolshevism, feudalism, slavery, etc.), autocratic religions (e.g., fundamentalist Islam, Roman Catholicism, etc.), patriarchy, heterosexism, white supremacy, and imperialism." Anarchist schools disagree on the methods by which these forms should be opposed. The principle of equal liberty is closer to anarchist political ethics in that it transcends both the liberal and socialist traditions. This entails that liberty and equality cannot be implemented within the state, resulting in the questioning of all forms of domination and hierarchy.Tactics Anarchists' tactics take various forms but in general serve two major goals, namely to first oppose the Establishment and secondly to promote anarchist ethics and reflect an anarchist vision of society, illustrating the unity of means and ends. A broad categorisation can be made between aims to destroy oppressive states and institutions by revolutionary means on one hand and aims to change society through evolutionary means on the other. Evolutionary tactics embrace nonviolence, reject violence and take a gradual approach to anarchist aims, although there is significant overlap between the two.Anarchist tactics have shifted during the course of the last century. Anarchists during the early 20th century focused more on strikes and militancy while contemporary anarchists use a broader array of approaches.Classical era tactics During the classical era, anarchists had a militant tendency. Not only did they confront state armed forces, as in Spain and Ukraine, but some of them also employed terrorism as propaganda of the deed. Assassination attempts were carried out against heads of state, some of which were successful. Anarchists also took part in revolutions. Many anarchists, especially the Galleanists, believed that these attempts would be the impetus for a revolution against capitalism and the state. Many of these attacks were done by individual assailants and the majority took place in the late 1870s, the early 1880s and the 1890s, with some still occurring in the early 1900s. Their decrease in prevalence was the result of further judicial power and targeting and cataloging by state institutions.Anarchist perspectives towards violence have always been controversial. Anarcho-pacifists advocate for non-violence means to achieve their stateless, nonviolent ends. Other anarchist groups advocate direct action, a tactic which can include acts of sabotage or terrorism. This attitude was quite prominent a century ago when seeing the state as a tyrant and some anarchists believing that they had every right to oppose its oppression by any means possible. Emma Goldman and Errico Malatesta, who were proponents of limited use of violence, stated that violence is merely a reaction to state violence as a necessary evil.Anarchists took an active role in strike actions, although they tended to be antipathetic to formal syndicalism, seeing it as reformist. They saw it as a part of the movement which sought to overthrow the state and capitalism. Anarchists also reinforced their propaganda within the arts, some of whom practiced naturism and nudism. Those anarchists also built communities which were based on friendship and were involved in the news media.Revolutionary tactics In the current era, Italian anarchist Alfredo Bonanno, a proponent of insurrectionary anarchism, has reinstated the debate on violence by rejecting the nonviolence tactic adopted since the late 19th century by Kropotkin and other prominent anarchists afterwards. Both Bonanno and the French group The Invisible Committee advocate for small, informal affiliation groups, where each member is responsible for their own actions but works together to bring down oppression utilizing sabotage and other violent means against state, capitalism, and other enemies. Members of The Invisible Committee were arrested in 2008 on various charges, terrorism included.Overall, contemporary anarchists are much less violent and militant than their ideological ancestors. They mostly engage in confronting the police during demonstrations and riots, especially in countries such as Canada, Greece, and Mexico. Militant black bloc protest groups are known for clashing with the police; however, anarchists not only clash with state operators, they also engage in the struggle against fascists and racists, taking anti-fascist action and mobilizing to prevent hate rallies from happening.Evolutionary tactics Anarchists commonly employ direct action. This can take the form of disrupting and protesting against unjust hierarchy, or the form of self-managing their lives through the creation of counter-institutions such as communes and non-hierarchical collectives. Decision-making is often handled in an anti-authoritarian way, with everyone having equal say in each decision, an approach known as horizontalism. Contemporary-era anarchists have been engaging with various grassroots movements that are more or less based on horizontalism, although not explicitly anarchist, respecting personal autonomy and participating in mass activism such as strikes and demonstrations. In contrast with the big-A anarchism of the classical era, the newly coined term small-a anarchism signals their tendency not to base their thoughts and actions on classical-era anarchism or to refer to classical anarchists such as Peter Kropotkin and Pierre-Joseph Proudhon to justify their opinions. Those anarchists would rather base their thought and praxis on their own experience which they will later theorize.The decision-making process of small anarchist affinity groups plays a significant tactical role. Anarchists have employed various methods in order to build a rough consensus among members of their group without the need of a leader or a leading group. One way is for an individual from the group to play the role of facilitator to help achieve a consensus without taking part in the discussion themselves or promoting a specific point. Minorities usually accept rough consensus, except when they feel the proposal contradicts anarchist ethics, goals and values. Anarchists usually form small groups (5–20 individuals) to enhance autonomy and friendships among their members. These kinds of groups more often than not interconnect with each other, forming larger networks. Anarchists still support and participate in strikes, especially wildcat strikes as these are leaderless strikes not organised centrally by a syndicate.As in the past, newspapers and journals are used, and anarchists have gone online in the World Wide Web to spread their message. Anarchists have found it easier to create websites because of distributional and other difficulties, hosting electronic libraries and other portals. Anarchists were also involved in developing various software that are available for free. The way these hacktivists work to develop and distribute resembles the anarchist ideals, especially when it comes to preserving users' privacy from state surveillance.Anarchists organize themselves to squat and reclaim public spaces. During important events such as protests and when spaces are being occupied, they are often called Temporary Autonomous Zones (TAZ), spaces where art, poetry, and surrealism are blended to display the anarchist ideal. As seen by anarchists, squatting is a way to regain urban space from the capitalist market, serving pragmatical needs and also being an exemplary direct action. Acquiring space enables anarchists to experiment with their ideas and build social bonds. Adding up these tactics while having in mind that not all anarchists share the same attitudes towards them, along with various forms of protesting at highly symbolic events, make up a carnivalesque atmosphere that is part of contemporary anarchist vividity.Key issues As anarchism is a philosophy that embodies many diverse attitudes, tendencies, and schools of thought; disagreement over questions of values, ideology, and tactics is common. Its diversity has led to widely different uses of identical terms among different anarchist traditions which has created a number of definitional concerns in anarchist theory. The compatibility of capitalism, nationalism, and religion with anarchism is widely disputed, and anarchism enjoys complex relationships with ideologies such as communism, collectivism, Marxism, and trade unionism. Anarchists may be motivated by humanism, divine authority, enlightened self-interest, veganism, or any number of alternative ethical doctrines. Phenomena such as civilisation, technology (e.g. within anarcho-primitivism), and the democratic process may be sharply criticised within some anarchist tendencies and simultaneously lauded in others.Gender, sexuality, and free love As gender and sexuality carry along them dynamics of hierarchy, many anarchists address, analyse, and oppose the suppression of one's autonomy imposed by gender roles.Sexuality was not often discussed by classical anarchists but the few that did felt that an anarchist society would lead to sexuality naturally developing. Sexual violence was a concern for anarchists such as Benjamin Tucker, who opposed age of consent laws, believing they would benefit predatory men. A historical current that arose and flourished during 1890 and 1920 within anarchism was free love. In contemporary anarchism, this current survives as a tendency to support polyamory and queer anarchism. Free love advocates were against marriage, which they saw as a way of men imposing authority over women, largely because marriage law greatly favoured the power of men. The notion of free love was much broader and included a critique of the established order that limited women's sexual freedom and pleasure. Those free love movements contributed to the establishment of communal houses, where large groups of travelers, anarchists and other activists slept in beds together. Free love had roots both in Europe and the United States; however, some anarchists struggled with the jealousy that arose from free love. Anarchist feminists were advocates of free love, against marriage, and pro-choice (utilising a contemporary term), and had a similar agenda. Anarchist and non-anarchist feminists differed on suffrage but were supportive of one another.During the second half of the 20th century, anarchism intermingled with the second wave of feminism, radicalising some currents of the feminist movement and being influenced as well. By the latest decades of the 20th century, anarchists and feminists were advocating for the rights and autonomy of women, gays, queers and other marginalised groups, with some feminist thinkers suggesting a fusion of the two currents. With the third wave of feminism, sexual identity and compulsory heterosexuality became a subject of study for anarchists, yielding a post-structuralist critique of sexual normality. Some anarchists distanced themselves from this line of thinking, suggesting that it leaned towards an individualism that was dropping the cause of social liberation.Anarchism and education The interest of anarchists in education stretches back to the first emergence of classical anarchism. Anarchists consider proper education, one which sets the foundations of the future autonomy of the individual and the society, to be an act of mutual aid. Anarchist writers such as William Godwin (Political Justice) and Max Stirner ("The False Principle of Our Education") attacked both state education and private education as another means by which the ruling class replicate their privileges.In 1901, Catalan anarchist and free thinker Francisco Ferrer established the Escuela Moderna in Barcelona as an opposition to the established education system which was dictated largely by the Catholic Church. Ferrer's approach was secular, rejecting both state and church involvement in the educational process whilst giving pupils large amounts of autonomy in planning their work and attendance. Ferrer aimed to educate the working class and explicitly sought to foster class consciousness among students. The school closed after constant harassment by the state and Ferrer was later arrested. Nonetheless, his ideas formed the inspiration for a series of modern schools around the world. Christian anarchist Leo Tolstoy, who published the essay Education and Culture, also established a similar school with its founding principle being that "for education to be effective it had to be free." In a similar token, A. S. Neill founded what became the Summerhill School in 1921, also declaring being free from coercion.Anarchist education is based largely on the idea that a child's right to develop freely and without manipulation ought to be respected and that rationality would lead children to morally good conclusions; however, there has been little consensus among anarchist figures as to what constitutes manipulation. Ferrer believed that moral indoctrination was necessary and explicitly taught pupils that equality, liberty and social justice were not possible under capitalism, along with other critiques of government and nationalism.Late 20th century and contemporary anarchist writers (Paul Goodman, Herbert Read, and Colin Ward) intensified and expanded the anarchist critique of state education, largely focusing on the need for a system that focuses on children's creativity rather than on their ability to attain a career or participate in consumerism as part of a consumer society. Contemporary anarchists such as Ward claim that state education serves to perpetuate socioeconomic inequality.While few anarchist education institutions have survived to the modern-day, major tenets of anarchist schools, among them respect for child autonomy and relying on reasoning rather than indoctrination as a teaching method, have spread among mainstream educational institutions. Judith Suissa names three schools as explicitly anarchists schools, namely the Free Skool Santa Cruz in the United States which is part of a wider American-Canadian network of schools, the Self-Managed Learning College in Brighton, England, and the Paideia School in Spain.Anarchism and the state Objection to the state and its institutions is a sine qua non of anarchism. Anarchists consider the state as a tool of domination and believe it to be illegitimate regardless of its political tendencies. Instead of people being able to control the aspects of their life, major decisions are taken by a small elite. Authority ultimately rests solely on power, regardless of whether that power is open or transparent, as it still has the ability to coerce people. Another anarchist argument against states is that the people constituting a government, even the most altruistic among officials, will unavoidably seek to gain more power, leading to corruption. Anarchists consider the idea that the state is the collective will of the people to be an unachievable fiction due to the fact that the ruling class is distinct from the rest of society.Specific anarchist attitudes towards the state vary. Robert Paul Wolff believed that the tension between authority and autonomy would mean the state could never be legitimate. Bakunin saw the state as meaning "coercion, domination by means of coercion, camouflaged if possible but unceremonious and overt if need be." A. John Simmons and Leslie Green, who leaned toward philosophical anarchism, believed that the state could be legitimate if it is governed by consensus, although they saw this as highly unlikely. Beliefs on how to abolish the state also differ.Anarchism and the arts The connection between anarchism and art was quite profound during the classical era of anarchism, especially among artistic currents that were developing during that era such as futurists, surrealists and others. In literature, anarchism was mostly associated with the New Apocalyptics and the neo-romanticism movement. In music, anarchism has been associated with music scenes such as punk. Anarchists such as Leo Tolstoy and Herbert Read stated that the border between the artist and the non-artist, what separates art from a daily act, is a construct produced by the alienation caused by capitalism and it prevents humans from living a joyful life.Other anarchists advocated for or used art as a means to achieve anarchist ends. In his book Breaking the Spell: A History of Anarchist Filmmakers, Videotape Guerrillas, and Digital Ninjas, Chris Robé claims that "anarchist-inflected practices have increasingly structured movement-based video activism." Throughout the 20th century, many prominent anarchists (Peter Kropotkin, Emma Goldman, Gustav Landauer and Camillo Berneri) and publications such as Anarchy wrote about matters pertaining to the arts.Three overlapping properties made art useful to anarchists. It could depict a critique of existing society and hierarchies, serve as a prefigurative tool to reflect the anarchist ideal society and even turn into a means of direct action such as in protests. As it appeals to both emotion and reason, art could appeal to the whole human and have a powerful effect. The 19th-century neo-impressionist movement had an ecological aesthetic and offered an example of an anarchist perception of the road towards socialism. In Les chataigniers a Osny by anarchist painter Camille Pissarro, the blending of aesthetic and social harmony is prefiguring an ideal anarchistic agrarian community.Analysis The most common critique of anarchism is that humans cannot self-govern and so a state is necessary for human survival. Philosopher Bertrand Russell supported this critique, stating that "[p]eace and war, tariffs, regulations of sanitary conditions and the sale of noxious drugs, the preservation of a just system of distribution: these, among others, are functions which could hardly be performed in a community in which there was no central government." Another common criticism of anarchism is that it fits a world of isolation in which only the small enough entities can be self-governing; a response would be that major anarchist thinkers advocated anarchist federalism.Philosophy lecturer Andrew G. Fiala composed a list of common arguments against anarchism which includes critiques such as that anarchism is innately related to violence and destruction, not only in the pragmatic world, such as at protests, but in the world of ethics as well. Secondly, anarchism is evaluated as unfeasible or utopian since the state cannot be defeated practically. This line of arguments most often calls for political action within the system to reform it. The third argument is that anarchism is self-contradictory. While it advocates for no-one to archiei, if accepted by the many, then anarchism would turn into the ruling political theory. In this line of criticism also comes the self-contradiction that anarchism calls for collective action whilst endorsing the autonomy of the individual, hence no collective action can be taken. Lastly, Fiala mentions a critique towards philosophical anarchism of being ineffective (all talk and thoughts) and in the meantime capitalism and bourgeois class remains strong.Philosophical anarchism has met the criticism of members of academia following the release of pro-anarchist books such as A. John Simmons' Moral Principles and Political Obligations. Law professor William A. Edmundson authored an essay to argue against three major philosophical anarchist principles which he finds fallacious. Edmundson says that while the individual does not owe the state a duty of obedience, this does not imply that anarchism is the inevitable conclusion and the state is still morally legitimate. In The Problem of Political Authority, Michael Huemer defends philosophical anarchism, claiming that "political authority is a moral illusion."One of the earliest criticisms is that anarchism defies and fails to understand the biological inclination to authority. Joseph Raz states that the acceptance of authority implies the belief that following their instructions will afford more success. Raz believes that this argument is true in following both authorities' successful and mistaken instruction. Anarchists reject this criticism because challenging or disobeying authority does not entail the disappearance of its advantages by acknowledging authority such as doctors or lawyers as reliable, nor does it involve a complete surrender of independent judgment. Anarchist perception of human nature, rejection of the state, and commitment to social revolution has been criticised by academics as naive, overly simplistic, and unrealistic, respectively. Classical anarchism has been criticised for relying too heavily on the belief that the abolition of the state will lead to human cooperation prospering.Friedrich Engels, considered to be one of the principal founders of Marxism, criticised anarchism's anti-authoritarianism as inherently counter-revolutionary because in his view a revolution is by itself authoritarian. Academic John Molyneux writes in his book Anarchism: A Marxist Criticism that "anarchism cannot win", believing that it lacks the ability to properly implement its ideas. The Marxist criticism of anarchism is that it has a utopian character because all individuals should have anarchist views and values. According to the Marxist view, that a social idea would follow directly from this human ideal and out of the free will of every individual formed its essence. Marxists state that this contradiction was responsible for their inability to act. In the anarchist vision, the conflict between liberty and equality was resolved through coexistence and intertwining.See also  Anarchism by country Governance without government List of anarchist political ideologies List of books about anarchismReferencesCitationsNotesSourcesPrimary sourcesSecondary sourcesTertiary sourcesFurther reading    Criticism of philosophical anarchism.   A defence of philosophical anarchism, stating that "both kinds of 'anarchism' [i.e. philosophical and political anarchism] are philosophical and political claims." (p. 137)  Anarchistic popular fiction novel.     An argument for philosophical anarchism.External links  Anarchy
[INFO|configuration_utils.py:731] 2024-07-31 19:21:34,400 >> loading configuration file /AI-DATA/Models/Llama/Meta-Llama-3-70B/config.json
[INFO|configuration_utils.py:796] 2024-07-31 19:21:34,402 >> Model config LlamaConfig {
  "_name_or_path": "/AI-DATA/Models/Llama/Meta-Llama-3-70B",
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "eos_token_id": 128001,
  "hidden_act": "silu",
  "hidden_size": 8192,
  "initializer_range": 0.02,
  "intermediate_size": 28672,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 64,
  "num_hidden_layers": 80,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.2",
  "use_cache": true,
  "vocab_size": 128256
}

[INFO|modeling_utils.py:3471] 2024-07-31 19:21:34,441 >> loading weights file /AI-DATA/Models/Llama/Meta-Llama-3-70B/model.safetensors.index.json
[INFO|modeling_utils.py:3614] 2024-07-31 19:21:34,444 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model
[WARNING|logging.py:329] 2024-07-31 19:21:34,447 >> You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
[WARNING|logging.py:329] 2024-07-31 19:21:34,447 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[WARNING|logging.py:329] 2024-07-31 19:21:34,453 >> Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[INFO|configuration_utils.py:962] 2024-07-31 19:21:34,453 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

[WARNING|logging.py:329] 2024-07-31 19:21:34,455 >> Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
mcToolsExtPid_UsedPort:418-38411
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
mcToolsExtPid_UsedPort:421-50205
mcToolsExtPid_UsedPort:420-36793
mcToolsExtPid_UsedPort:419-44993
mcToolsExtPid_UsedPort:423-42683
mcToolsExtPid_UsedPort:425-51735
mcToolsExtPid_UsedPort:424-51731
mcToolsExtPid_UsedPort:422-35593
[2024-07-31 19:21:39,379] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 723, num_elems = 70.55B
Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:00<00:07,  3.78it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:00<00:07,  3.75it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:00<00:07,  3.74it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:00<00:08,  3.62it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:00<00:09,  3.11it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:00<00:05,  4.89it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:00<00:07,  3.70it/s]Loading checkpoint shards:   7%|▋         | 2/30 [00:00<00:09,  2.88it/s]Loading checkpoint shards:   7%|▋         | 2/30 [00:00<00:08,  3.38it/s]Loading checkpoint shards:   7%|▋         | 2/30 [00:00<00:10,  2.69it/s]Loading checkpoint shards:   7%|▋         | 2/30 [00:00<00:10,  2.69it/s]Loading checkpoint shards:   7%|▋         | 2/30 [00:00<00:10,  2.69it/s]Loading checkpoint shards:   7%|▋         | 2/30 [00:00<00:11,  2.52it/s]Loading checkpoint shards:   7%|▋         | 2/30 [00:00<00:09,  2.88it/s]Loading checkpoint shards:  10%|█         | 3/30 [00:01<00:10,  2.64it/s]Loading checkpoint shards:  10%|█         | 3/30 [00:01<00:12,  2.25it/s]Loading checkpoint shards:  10%|█         | 3/30 [00:01<00:11,  2.28it/s]Loading checkpoint shards:  10%|█         | 3/30 [00:01<00:12,  2.21it/s]Loading checkpoint shards:  10%|█         | 3/30 [00:01<00:12,  2.20it/s]Loading checkpoint shards:  10%|█         | 3/30 [00:01<00:11,  2.40it/s]Loading checkpoint shards:  10%|█         | 3/30 [00:01<00:12,  2.14it/s][19:21:40.846][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[19:21:40.846][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 134217728 bytes from vram
[19:21:40.846][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x8000000, device 0x6e487a0
[19:21:40.846][MCR][E]mx_device.cpp            :2162: Failed creating memory
[19:21:40.846][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[19:21:40.846][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :134217728 | free :0 | total :68283269120 
[19:21:40.846][MCR][E]mc_runtime_api_deprecated.cpp:306 : 421  : [7f321ee506c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
[19:21:40.875][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[19:21:40.875][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[19:21:40.875][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x783a5f0
[19:21:40.875][MCR][E]mx_device.cpp            :2162: Failed creating memory
[19:21:40.875][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[19:21:40.875][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[19:21:40.875][MCR][E]mc_runtime_api_deprecated.cpp:306 : 420  : [7fc29734f6c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
[19:21:40.918][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[19:21:40.918][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[19:21:40.918][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x78a8720
[19:21:40.918][MCR][E]mx_device.cpp            :2162: Failed creating memory
[19:21:40.918][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[19:21:40.918][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[19:21:40.918][MCR][E]mc_runtime_api_deprecated.cpp:306 : 419  : [7f6ca936d6c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
[19:21:40.921][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[19:21:40.921][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 134217728 bytes from vram
[19:21:40.921][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x8000000, device 0x7c57590
[19:21:40.921][MCR][E]mx_device.cpp            :2162: Failed creating memory
[19:21:40.921][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[19:21:40.921][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :134217728 | free :0 | total :68316823552 
[19:21:40.921][MCR][E]mc_runtime_api_deprecated.cpp:306 : 422  : [7f264126b6c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
[19:21:40.940][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:21:40.940][MCR][E]mx_device.cpp            :1872: Failed to allow iolink access for memory allocation, size 0x8000000, device 0x6cd3ee0
[19:21:40.940][MXKW][E]memory.c                :309 : [mxkwFreeMemory]fmm_release_svm failed
[19:21:40.940][MXC][E]mxkwFreeMemory ptr(0x7f49dee00000), size(134217728), status: 1
[19:21:40.940][MCR][E]mx_device.cpp            :2162: Failed creating memory
[19:21:40.940][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[19:21:40.940][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :134217728 | free :3415347200 | total :68283269120 
[19:21:40.940][MCR][E]mc_runtime_api_deprecated.cpp:306 : 424  : [7f5ca4f1f6c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
[19:21:40.941][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:21:40.941][MCR][E]mx_device.cpp            :1872: Failed to allow iolink access for memory allocation, size 0x1c000000, device 0x8027300
[19:21:40.941][MXKW][E]memory.c                :309 : [mxkwFreeMemory]fmm_release_svm failed
[19:21:40.941][MXC][E]mxkwFreeMemory ptr(0x7f2960a00000), size(469762048), status: 1
[19:21:40.941][MCR][E]mx_device.cpp            :2162: Failed creating memory
[19:21:40.941][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[19:21:40.941][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[19:21:40.941][MCR][E]mc_runtime_api_deprecated.cpp:306 : 425  : [7f3ce77986c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
[19:21:40.942][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:21:40.942][MCR][E]mx_device.cpp            :1872: Failed to allow iolink access for memory allocation, size 0x1c000000, device 0x7b7f770
[19:21:40.943][MXKW][E]memory.c                :309 : [mxkwFreeMemory]fmm_release_svm failed
[19:21:40.943][MXC][E]mxkwFreeMemory ptr(0x7fc34a600000), size(469762048), status: 1
[19:21:40.943][MCR][E]mx_device.cpp            :2162: Failed creating memory
[19:21:40.943][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[19:21:40.943][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[19:21:40.943][MCR][E]mc_runtime_api_deprecated.cpp:306 : 423  : [7fd6e6bb86c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
[19:21:41.000][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:21:41.001][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:21:41.001][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:21:41.001][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:21:41.438][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:21:41.438][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:21:41.438][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:21:41.438][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:21:41.963][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:21:41.963][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:21:41.963][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:21:41.963][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:21:42.377][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:21:42.377][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:21:42.377][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:21:42.377][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:21:42.776][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:21:42.776][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:21:42.776][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:21:42.776][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:21:43.190][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:21:43.190][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:21:43.190][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:21:43.190][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:21:43.606][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:21:43.606][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:21:43.606][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:21:43.606][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:21:44.130][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:21:44.130][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:21:44.130][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:21:44.130][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:21:44.548][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:21:44.548][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:21:44.548][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:21:44.548][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
Loading checkpoint shards:   3%|▎         | 1/30 [00:05<02:33,  5.30s/it][19:21:45.205][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:21:45.205][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:21:45.205][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:21:45.205][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:21:45.636][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:21:45.636][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:21:45.636][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:21:45.636][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:21:46.040][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:21:46.040][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:21:46.040][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:21:46.040][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:21:46.548][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:21:46.548][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:21:46.548][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:21:46.548][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:21:46.940][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:21:46.940][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:21:46.940][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:21:46.940][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:21:47.367][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:21:47.367][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:21:47.367][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:21:47.367][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:21:47.774][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:21:47.774][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:21:47.774][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:21:47.774][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:21:55.994][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:21:55.994][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:21:55.994][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:21:55.994][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:22:08.997][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:22:08.997][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:22:08.997][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:22:08.997][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:22:30.211][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:22:30.211][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:22:30.211][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:22:30.211][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:22:36.537][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:22:36.537][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:22:36.537][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:22:36.537][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:22:43.509][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:22:43.509][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:22:43.509][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:22:43.509][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:22:50.898][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:22:50.898][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:22:50.898][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:22:50.898][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:22:59.787][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:22:59.787][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:22:59.787][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:22:59.787][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
Loading checkpoint shards:   7%|▋         | 2/30 [01:20<21:40, 46.45s/it][19:23:21.411][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:23:21.411][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:23:21.411][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:23:21.411][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:23:32.421][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:23:32.421][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:23:32.421][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:23:32.421][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:23:48.790][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:23:48.790][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:23:48.790][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:23:48.790][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:24:02.823][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:24:02.823][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:24:02.823][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:24:02.823][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:24:17.881][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:24:17.881][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:24:17.881][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:24:17.881][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:24:30.868][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:24:30.868][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:24:30.868][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:24:30.868][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:24:42.350][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:24:42.350][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:24:42.350][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:24:42.350][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:25:03.032][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:25:03.032][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:25:03.032][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:25:03.032][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:25:14.166][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:25:14.166][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:25:14.166][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:25:14.166][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:25:33.871][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:25:33.871][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:25:33.871][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:25:33.871][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:25:51.845][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:25:51.845][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:25:51.845][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:25:51.845][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:26:02.728][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:26:02.728][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:26:02.728][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:26:02.728][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:26:12.567][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:26:12.567][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:26:12.567][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:26:12.567][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:26:24.448][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:26:24.448][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:26:24.448][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:26:24.448][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
Loading checkpoint shards:  10%|█         | 3/30 [04:47<53:50, 119.63s/it][19:26:58.011][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:26:58.011][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:26:58.011][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:26:58.011][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:27:50.106][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:27:50.106][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:27:50.106][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:27:50.106][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:28:25.230][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:28:25.230][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:28:25.230][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:28:25.230][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:28:59.133][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:28:59.133][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:28:59.133][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:28:59.133][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:29:38.556][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:29:38.556][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:29:38.556][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:29:38.556][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
Loading checkpoint shards:  13%|█▎        | 4/30 [08:06<1:23:09, 191.91s/it]Loading checkpoint shards:  17%|█▋        | 5/30 [08:08<51:27, 123.50s/it]  Loading checkpoint shards:  20%|██        | 6/30 [08:09<32:43, 81.80s/it] [19:29:50.887][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:29:50.887][MCR][E]mx_device.cpp            :1872: Failed to allow iolink access for memory allocation, size 0x1c000000, device 0x6cd3ee0
[19:29:50.887][MXKW][E]memory.c                :309 : [mxkwFreeMemory]fmm_release_svm failed
[19:29:50.887][MXC][E]mxkwFreeMemory ptr(0x7f49a6a00000), size(469762048), status: 1
[19:29:50.887][MCR][E]mx_device.cpp            :2162: Failed creating memory
[19:29:50.887][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[19:29:50.887][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :3073105920 | total :68283269120 
[19:29:50.887][MCR][E]mc_runtime_api_deprecated.cpp:306 : 424  : [7f5ca4f1f6c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
[19:29:52.987][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:29:52.987][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:29:52.987][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:29:52.987][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:30:01.333][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:30:01.333][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:30:01.333][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:30:01.333][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
[19:30:10.296][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:30:10.296][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:30:10.296][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:30:10.296][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
Loading checkpoint shards:  13%|█▎        | 4/30 [08:31<1:27:21, 201.59s/it]Loading checkpoint shards:  13%|█▎        | 4/30 [08:31<1:27:21, 201.59s/it]Loading checkpoint shards:  13%|█▎        | 4/30 [08:31<1:27:23, 201.66s/it]Loading checkpoint shards:  13%|█▎        | 4/30 [08:31<1:27:24, 201.70s/it]Loading checkpoint shards:  17%|█▋        | 5/30 [08:31<53:45, 129.04s/it]  Loading checkpoint shards:  17%|█▋        | 5/30 [08:31<53:45, 129.04s/it]  Loading checkpoint shards:  17%|█▋        | 5/30 [08:31<53:47, 129.10s/it]  Loading checkpoint shards:  17%|█▋        | 5/30 [08:31<53:48, 129.12s/it]  Loading checkpoint shards:  20%|██        | 6/30 [08:32<34:07, 85.30s/it] Loading checkpoint shards:  20%|██        | 6/30 [08:32<34:07, 85.30s/it] [19:30:11.759][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:30:11.759][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:30:11.759][MCR][E]mx_device.cpp            :1872: Failed to allow iolink access for memory allocation, size 0x8000000, device 0x7b7f770
[19:30:11.759][MCR][E]mx_device.cpp            :1872: Failed to allow iolink access for memory allocation, size 0x8000000, device 0x8027300
[19:30:11.759][MXKW][E]memory.c                :309 : [mxkwFreeMemory]fmm_release_svm failed
[19:30:11.759][MXKW][E]memory.c                :309 : [mxkwFreeMemory]fmm_release_svm failed
[19:30:11.759][MXC][E]mxkwFreeMemory ptr(0x7fc374c00000), size(134217728), status: 1
[19:30:11.759][MXC][E]mxkwFreeMemory ptr(0x7f2988c00000), size(134217728), status: 1
[19:30:11.759][MCR][E]mx_device.cpp            :2162: Failed creating memory
[19:30:11.759][MCR][E]mx_device.cpp            :2162: Failed creating memory
[19:30:11.759][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[19:30:11.759][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[19:30:11.759][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :134217728 | free :0 | total :68283269120 
[19:30:11.759][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :134217728 | free :0 | total :68283269120 
[19:30:11.759][MCR][E]mc_runtime_api_deprecated.cpp:306 : 425  : [7f3ce77986c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
[19:30:11.759][MCR][E]mc_runtime_api_deprecated.cpp:306 : 423  : [7fd6e6bb86c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  20%|██        | 6/30 [08:32<34:08, 85.36s/it] [19:30:11.830][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:30:11.830][MCR][E]mx_device.cpp            :1872: Failed to allow iolink access for memory allocation, size 0x1c000000, device 0x78a8720
[19:30:11.830][MXKW][E]memory.c                :309 : [mxkwFreeMemory]fmm_release_svm failed
[19:30:11.830][MXC][E]mxkwFreeMemory ptr(0x7f5fa4c00000), size(469762048), status: 1
[19:30:11.830][MCR][E]mx_device.cpp            :2162: Failed creating memory
[19:30:11.830][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[19:30:11.830][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :12907577344 | total :68283269120 
[19:30:11.830][MCR][E]mc_runtime_api_deprecated.cpp:306 : 419  : [7f6ca936d6c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  20%|██        | 6/30 [08:32<34:08, 85.37s/it] [19:30:11.838][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:30:11.838][MCR][E]mx_device.cpp            :1872: Failed to allow iolink access for memory allocation, size 0x1c000000, device 0x783a5f0
[19:30:11.838][MXKW][E]memory.c                :309 : [mxkwFreeMemory]fmm_release_svm failed
[19:30:11.838][MXC][E]mxkwFreeMemory ptr(0x7fb574a00000), size(469762048), status: 1
[19:30:11.838][MCR][E]mx_device.cpp            :2162: Failed creating memory
[19:30:11.838][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[19:30:11.838][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :12639137792 | total :68283269120 
[19:30:11.838][MCR][E]mc_runtime_api_deprecated.cpp:306 : 420  : [7fc29734f6c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
[19:30:15.453][MXKW][E]fmm.c                   :2863: [_fmm_map_to_gpu]error at kmtIoctl(mxcd_fd, MXCD_IOC_MAP_GPU_MEMORY, &args) ret = -1 
[19:30:15.453][MCR][E]mx_blit.cpp              :74  : Failed to lock memory to pool, failed with mxc_status: 4096.
[19:30:15.453][MCR][E]mx_blit.cpp              :753 : BlitManager::writeBuffer failed to pin a resource!
[19:30:15.453][MCR][E]mx_device.cpp            :6157: submitWriteMemory failed!
Loading checkpoint shards:  13%|█▎        | 4/30 [08:36<1:28:13, 203.61s/it]Loading checkpoint shards:  13%|█▎        | 4/30 [08:36<1:28:14, 203.62s/it]Loading checkpoint shards:  17%|█▋        | 5/30 [08:36<54:17, 130.31s/it]  Loading checkpoint shards:  17%|█▋        | 5/30 [08:36<54:17, 130.31s/it]  Loading checkpoint shards:  20%|██        | 6/30 [08:36<34:26, 86.12s/it] Loading checkpoint shards:  20%|██        | 6/30 [08:36<34:26, 86.12s/it] Loading checkpoint shards:  23%|██▎       | 7/30 [08:37<22:15, 58.07s/it]Loading checkpoint shards:  23%|██▎       | 7/30 [08:37<22:15, 58.07s/it][19:30:18.087][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[19:30:18.087][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[19:30:18.087][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x6e487a0
[19:30:18.087][MCR][E]mx_device.cpp            :2162: Failed creating memory
[19:30:18.087][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[19:30:18.087][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[19:30:18.087][MCR][E]mc_runtime_api_deprecated.cpp:306 : 421  : [7f321ee506c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
[19:30:18.092][MCR][E]mx_device.cpp            :2162: Failed creating memory
[19:30:18.092][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[19:30:18.092][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68316823552 
[19:30:18.092][MCR][E]mc_runtime_api_deprecated.cpp:306 : 422  : [7f264126b6c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  13%|█▎        | 4/30 [12:22<1:49:19, 252.28s/it]Loading checkpoint shards:  17%|█▋        | 5/30 [18:20<2:00:56, 290.24s/it]Loading checkpoint shards:  20%|██        | 6/30 [24:08<2:03:55, 309.83s/it]Loading checkpoint shards:  23%|██▎       | 7/30 [24:08<2:19:20, 363.51s/it]Loading checkpoint shards:  23%|██▎       | 7/30 [24:08<2:19:20, 363.50s/it]Loading checkpoint shards:  27%|██▋       | 8/30 [24:08<1:30:54, 247.94s/it]Loading checkpoint shards:  27%|██▋       | 8/30 [24:08<1:30:54, 247.93s/it]Loading checkpoint shards:  30%|███       | 9/30 [24:09<59:45, 170.73s/it]  Loading checkpoint shards:  30%|███       | 9/30 [24:09<59:45, 170.72s/it]  [19:45:49.852][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[19:45:49.852][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[19:45:49.852][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x78a8720
[19:45:49.852][MCR][E]mx_device.cpp            :2162: Failed creating memory
[19:45:49.852][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[19:45:49.853][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[19:45:49.853][MCR][E]mc_runtime_api_deprecated.cpp:306 : 419  : [7f6ca936d6c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
[19:45:49.857][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[19:45:49.857][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[19:45:49.857][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x783a5f0
[19:45:49.857][MCR][E]mx_device.cpp            :2162: Failed creating memory
[19:45:49.857][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[19:45:49.857][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[19:45:49.857][MCR][E]mc_runtime_api_deprecated.cpp:306 : 420  : [7fc29734f6c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  23%|██▎       | 7/30 [25:44<2:33:14, 399.77s/it]Loading checkpoint shards:  27%|██▋       | 8/30 [25:44<1:39:57, 272.61s/it]Loading checkpoint shards:  30%|███       | 9/30 [25:44<1:05:37, 187.49s/it][19:47:24.491][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[19:47:24.491][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[19:47:24.491][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x6cd3ee0
[19:47:24.491][MCR][E]mx_device.cpp            :2162: Failed creating memory
[19:47:24.491][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[19:47:24.491][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[19:47:24.491][MCR][E]mc_runtime_api_deprecated.cpp:306 : 424  : [7f5ca4f1f6c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  23%|██▎       | 7/30 [28:40<2:53:27, 452.49s/it]Loading checkpoint shards:  23%|██▎       | 7/30 [28:40<2:53:27, 452.49s/it]Loading checkpoint shards:  27%|██▋       | 8/30 [28:40<1:53:08, 308.57s/it]Loading checkpoint shards:  27%|██▋       | 8/30 [28:40<1:53:08, 308.57s/it]Loading checkpoint shards:  30%|███       | 9/30 [28:41<1:14:17, 212.24s/it]Loading checkpoint shards:  30%|███       | 9/30 [28:41<1:14:17, 212.24s/it]Loading checkpoint shards:  33%|███▎      | 10/30 [28:41<48:56, 146.85s/it] Loading checkpoint shards:  33%|███▎      | 10/30 [28:41<48:56, 146.85s/it] [19:50:21.817][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[19:50:21.817][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[19:50:21.817][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x8027300
[19:50:21.817][MCR][E]mx_device.cpp            :2162: Failed creating memory
[19:50:21.817][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[19:50:21.817][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[19:50:21.817][MCR][E]mc_runtime_api_deprecated.cpp:306 : 425  : [7f3ce77986c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
[19:50:21.818][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[19:50:21.818][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[19:50:21.818][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x7b7f770
[19:50:21.818][MCR][E]mx_device.cpp            :2162: Failed creating memory
[19:50:21.818][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[19:50:21.818][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[19:50:21.818][MCR][E]mc_runtime_api_deprecated.cpp:306 : 423  : [7fd6e6bb86c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  23%|██▎       | 7/30 [28:58<1:56:18, 303.42s/it]Loading checkpoint shards:  27%|██▋       | 8/30 [28:58<2:37:06, 428.50s/it]Loading checkpoint shards:  30%|███       | 9/30 [28:59<1:43:07, 294.64s/it]Loading checkpoint shards:  33%|███▎      | 10/30 [28:59<1:07:55, 203.77s/it][19:50:39.141][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[19:50:39.141][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[19:50:39.141][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x6e487a0
[19:50:39.141][MCR][E]mx_device.cpp            :2162: Failed creating memory
[19:50:39.141][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[19:50:39.141][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[19:50:39.141][MCR][E]mc_runtime_api_deprecated.cpp:306 : 421  : [7f321ee506c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  27%|██▋       | 8/30 [29:34<2:41:16, 439.86s/it]Loading checkpoint shards:  30%|███       | 9/30 [29:34<1:45:51, 302.45s/it]Loading checkpoint shards:  33%|███▎      | 10/30 [29:35<1:09:43, 209.17s/it][19:51:14.849][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[19:51:14.849][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 134217728 bytes from vram
[19:51:14.849][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x8000000, device 0x7c57590
[19:51:14.849][MCR][E]mx_device.cpp            :2162: Failed creating memory
[19:51:14.849][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[19:51:14.849][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :134217728 | free :0 | total :68316823552 
[19:51:14.849][MCR][E]mc_runtime_api_deprecated.cpp:306 : 422  : [7f264126b6c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  27%|██▋       | 8/30 [34:30<1:54:38, 312.65s/it]Loading checkpoint shards:  30%|███       | 9/30 [35:43<1:23:11, 237.71s/it]Loading checkpoint shards:  33%|███▎      | 10/30 [35:51<1:51:31, 334.56s/it]Loading checkpoint shards:  33%|███▎      | 10/30 [35:51<1:51:32, 334.61s/it]Loading checkpoint shards:  37%|███▋      | 11/30 [35:51<1:13:33, 232.28s/it]Loading checkpoint shards:  37%|███▋      | 11/30 [35:51<1:13:34, 232.32s/it]Loading checkpoint shards:  40%|████      | 12/30 [35:52<48:33, 161.84s/it]  Loading checkpoint shards:  40%|████      | 12/30 [35:52<48:33, 161.87s/it]  [19:57:32.609][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[19:57:32.609][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[19:57:32.609][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[19:57:32.609][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x783a5f0
[19:57:32.609][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[19:57:32.609][MCR][E]mx_device.cpp            :2162: Failed creating memory
[19:57:32.609][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x78a8720
[19:57:32.609][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[19:57:32.609][MCR][E]mx_device.cpp            :2162: Failed creating memory
[19:57:32.609][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[19:57:32.609][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[19:57:32.609][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[19:57:32.609][MCR][E]mc_runtime_api_deprecated.cpp:306 : 419  : [7f6ca936d6c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
[19:57:32.609][MCR][E]mc_runtime_api_deprecated.cpp:306 : 420  : [7fc29734f6c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  33%|███▎      | 10/30 [36:01<1:46:40, 320.02s/it]Loading checkpoint shards:  37%|███▋      | 11/30 [36:01<1:10:21, 222.19s/it]Loading checkpoint shards:  40%|████      | 12/30 [36:02<46:24, 154.70s/it]  Loading checkpoint shards:  43%|████▎     | 13/30 [36:02<30:35, 107.94s/it][19:57:42.501][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[19:57:42.501][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[19:57:42.501][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x6cd3ee0
[19:57:42.501][MCR][E]mx_device.cpp            :2162: Failed creating memory
[19:57:42.501][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[19:57:42.501][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[19:57:42.501][MCR][E]mc_runtime_api_deprecated.cpp:306 : 424  : [7f5ca4f1f6c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  33%|███▎      | 10/30 [36:11<57:36, 172.84s/it] Loading checkpoint shards:  37%|███▋      | 11/30 [36:16<1:16:20, 241.05s/it]Loading checkpoint shards:  37%|███▋      | 11/30 [36:16<1:16:20, 241.05s/it]Loading checkpoint shards:  40%|████      | 12/30 [36:16<50:21, 167.84s/it]  Loading checkpoint shards:  40%|████      | 12/30 [36:16<50:21, 167.85s/it]  Loading checkpoint shards:  43%|████▎     | 13/30 [36:17<33:10, 117.11s/it]Loading checkpoint shards:  43%|████▎     | 13/30 [36:17<33:11, 117.13s/it][19:57:56.739][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[19:57:56.739][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[19:57:56.739][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x7b7f770
[19:57:56.739][MCR][E]mx_device.cpp            :2162: Failed creating memory
[19:57:56.739][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[19:57:56.739][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[19:57:56.739][MCR][E]mc_runtime_api_deprecated.cpp:306 : 423  : [7fd6e6bb86c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
[19:57:56.820][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[19:57:56.820][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[19:57:56.820][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x8027300
[19:57:56.820][MCR][E]mx_device.cpp            :2162: Failed creating memory
[19:57:56.820][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[19:57:56.820][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[19:57:56.820][MCR][E]mc_runtime_api_deprecated.cpp:306 : 425  : [7f3ce77986c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  37%|███▋      | 11/30 [36:21<1:27:37, 276.73s/it]Loading checkpoint shards:  40%|████      | 12/30 [36:21<57:47, 192.65s/it]  Loading checkpoint shards:  43%|████▎     | 13/30 [36:22<38:04, 134.39s/it][19:58:02.057][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[19:58:02.057][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[19:58:02.057][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x6e487a0
[19:58:02.057][MCR][E]mx_device.cpp            :2162: Failed creating memory
[19:58:02.057][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[19:58:02.057][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[19:58:02.057][MCR][E]mc_runtime_api_deprecated.cpp:306 : 421  : [7f321ee506c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  37%|███▋      | 11/30 [36:24<1:25:37, 270.39s/it]Loading checkpoint shards:  40%|████      | 12/30 [36:24<56:28, 188.23s/it]  Loading checkpoint shards:  43%|████▎     | 13/30 [36:25<37:12, 131.30s/it][19:58:04.751][MCR][E]mx_device.cpp            :2162: Failed creating memory
[19:58:04.751][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[19:58:04.751][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68316823552 
[19:58:04.751][MCR][E]mc_runtime_api_deprecated.cpp:306 : 422  : [7f264126b6c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  37%|███▋      | 11/30 [36:38<40:35, 128.16s/it]Loading checkpoint shards:  40%|████      | 12/30 [37:04<29:10, 97.24s/it] Loading checkpoint shards:  43%|████▎     | 13/30 [37:24<39:51, 140.70s/it]Loading checkpoint shards:  43%|████▎     | 13/30 [37:24<39:51, 140.69s/it]Loading checkpoint shards:  47%|████▋     | 14/30 [37:24<26:13, 98.33s/it] Loading checkpoint shards:  47%|████▋     | 14/30 [37:24<26:13, 98.32s/it] Loading checkpoint shards:  50%|█████     | 15/30 [37:25<17:14, 68.95s/it]Loading checkpoint shards:  50%|█████     | 15/30 [37:25<17:14, 68.95s/it]Loading checkpoint shards:  53%|█████▎    | 16/30 [37:27<11:20, 48.62s/it]Loading checkpoint shards:  53%|█████▎    | 16/30 [37:27<11:20, 48.61s/it][19:59:07.080][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[19:59:07.080][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[19:59:07.080][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x78a8720
[19:59:07.080][MCR][E]mx_device.cpp            :2162: Failed creating memory
[19:59:07.080][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[19:59:07.080][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[19:59:07.080][MCR][E]mc_runtime_api_deprecated.cpp:306 : 419  : [7f6ca936d6c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
[19:59:07.086][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[19:59:07.086][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[19:59:07.086][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x783a5f0
[19:59:07.086][MCR][E]mx_device.cpp            :2162: Failed creating memory
[19:59:07.086][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[19:59:07.086][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[19:59:07.086][MCR][E]mc_runtime_api_deprecated.cpp:306 : 420  : [7fc29734f6c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  43%|████▎     | 13/30 [37:34<21:44, 76.74s/it]Loading checkpoint shards:  47%|████▋     | 14/30 [37:35<27:34, 103.43s/it]Loading checkpoint shards:  50%|█████     | 15/30 [37:36<18:05, 72.36s/it] Loading checkpoint shards:  53%|█████▎    | 16/30 [37:36<11:49, 50.68s/it][19:59:15.961][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[19:59:15.961][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[19:59:15.961][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x6cd3ee0
[19:59:15.961][MCR][E]mx_device.cpp            :2162: Failed creating memory
[19:59:15.961][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[19:59:15.962][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[19:59:15.962][MCR][E]mc_runtime_api_deprecated.cpp:306 : 424  : [7f5ca4f1f6c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  47%|████▋     | 14/30 [37:47<29:06, 109.17s/it]Loading checkpoint shards:  47%|████▋     | 14/30 [37:48<29:07, 109.22s/it]Loading checkpoint shards:  50%|█████     | 15/30 [37:48<19:05, 76.37s/it] Loading checkpoint shards:  50%|█████     | 15/30 [37:48<19:06, 76.40s/it] Loading checkpoint shards:  53%|█████▎    | 16/30 [37:48<12:28, 53.49s/it][19:59:28.348][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[19:59:28.348][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[19:59:28.348][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x7b7f770
[19:59:28.348][MCR][E]mx_device.cpp            :2162: Failed creating memory
[19:59:28.348][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[19:59:28.348][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[19:59:28.348][MCR][E]mc_runtime_api_deprecated.cpp:306 : 423  : [7fd6e6bb86c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  53%|█████▎    | 16/30 [37:48<12:29, 53.51s/it][19:59:28.532][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[19:59:28.532][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[19:59:28.532][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x8027300
[19:59:28.532][MCR][E]mx_device.cpp            :2162: Failed creating memory
[19:59:28.532][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[19:59:28.532][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[19:59:28.532][MCR][E]mc_runtime_api_deprecated.cpp:306 : 425  : [7f3ce77986c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  47%|████▋     | 14/30 [37:55<32:28, 121.80s/it]Loading checkpoint shards:  50%|█████     | 15/30 [37:55<21:18, 85.21s/it] Loading checkpoint shards:  53%|█████▎    | 16/30 [37:55<13:55, 59.66s/it]Loading checkpoint shards:  57%|█████▋    | 17/30 [37:56<09:03, 41.81s/it][19:59:36.017][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[19:59:36.017][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[19:59:36.017][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x6e487a0
[19:59:36.017][MCR][E]mx_device.cpp            :2162: Failed creating memory
[19:59:36.017][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[19:59:36.017][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[19:59:36.017][MCR][E]mc_runtime_api_deprecated.cpp:306 : 421  : [7f321ee506c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  47%|████▋     | 14/30 [37:58<31:56, 119.77s/it]Loading checkpoint shards:  50%|█████     | 15/30 [37:58<20:56, 83.75s/it] Loading checkpoint shards:  53%|█████▎    | 16/30 [37:58<13:40, 58.63s/it]Loading checkpoint shards:  57%|█████▋    | 17/30 [37:59<08:54, 41.09s/it][19:59:38.622][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[19:59:38.622][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 134217728 bytes from vram
[19:59:38.622][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x8000000, device 0x7c57590
[19:59:38.622][MCR][E]mx_device.cpp            :2162: Failed creating memory
[19:59:38.622][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[19:59:38.622][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :134217728 | free :0 | total :68316823552 
[19:59:38.622][MCR][E]mc_runtime_api_deprecated.cpp:306 : 422  : [7f264126b6c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  47%|████▋     | 14/30 [38:03<16:38, 62.40s/it]Loading checkpoint shards:  50%|█████     | 15/30 [38:30<12:54, 51.61s/it]Loading checkpoint shards:  53%|█████▎    | 16/30 [38:56<10:17, 44.10s/it]Loading checkpoint shards:  57%|█████▋    | 17/30 [38:57<13:13, 61.04s/it]Loading checkpoint shards:  57%|█████▋    | 17/30 [38:57<13:14, 61.13s/it]Loading checkpoint shards:  60%|██████    | 18/30 [38:57<08:33, 42.83s/it]Loading checkpoint shards:  60%|██████    | 18/30 [38:57<08:34, 42.88s/it]Loading checkpoint shards:  63%|██████▎   | 19/30 [38:58<05:31, 30.14s/it]Loading checkpoint shards:  63%|██████▎   | 19/30 [38:58<05:32, 30.19s/it][20:00:38.146][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[20:00:38.146][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[20:00:38.146][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x78a8720
[20:00:38.146][MCR][E]mx_device.cpp            :2162: Failed creating memory
[20:00:38.146][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[20:00:38.146][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[20:00:38.146][MCR][E]mc_runtime_api_deprecated.cpp:306 : 419  : [7f6ca936d6c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
[20:00:38.150][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[20:00:38.150][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[20:00:38.151][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x783a5f0
[20:00:38.151][MCR][E]mx_device.cpp            :2162: Failed creating memory
[20:00:38.151][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[20:00:38.151][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[20:00:38.151][MCR][E]mc_runtime_api_deprecated.cpp:306 : 420  : [7fc29734f6c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  57%|█████▋    | 17/30 [39:07<13:37, 62.86s/it]Loading checkpoint shards:  60%|██████    | 18/30 [39:07<08:48, 44.08s/it]Loading checkpoint shards:  63%|██████▎   | 19/30 [39:08<05:40, 30.94s/it][20:00:47.885][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[20:00:47.885][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[20:00:47.885][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x6cd3ee0
[20:00:47.885][MCR][E]mx_device.cpp            :2162: Failed creating memory
[20:00:47.885][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[20:00:47.885][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[20:00:47.885][MCR][E]mc_runtime_api_deprecated.cpp:306 : 424  : [7f5ca4f1f6c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  57%|█████▋    | 17/30 [39:19<14:02, 64.79s/it]Loading checkpoint shards:  57%|█████▋    | 17/30 [39:19<14:02, 64.80s/it]Loading checkpoint shards:  60%|██████    | 18/30 [39:20<09:05, 45.42s/it]Loading checkpoint shards:  60%|██████    | 18/30 [39:20<09:05, 45.43s/it]Loading checkpoint shards:  63%|██████▎   | 19/30 [39:20<05:50, 31.88s/it]Loading checkpoint shards:  63%|██████▎   | 19/30 [39:20<05:50, 31.89s/it]Loading checkpoint shards:  67%|██████▋   | 20/30 [39:20<03:44, 22.41s/it]Loading checkpoint shards:  67%|██████▋   | 20/30 [39:20<03:44, 22.41s/it][20:01:00.791][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[20:01:00.791][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 134217728 bytes from vram
[20:01:00.791][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x8000000, device 0x7b7f770
[20:01:00.791][MCR][E]mx_device.cpp            :2162: Failed creating memory
[20:01:00.791][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[20:01:00.791][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :134217728 | free :0 | total :68283269120 
[20:01:00.791][MCR][E]mc_runtime_api_deprecated.cpp:306 : 423  : [7fd6e6bb86c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
[20:01:00.793][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[20:01:00.793][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[20:01:00.793][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x8027300
[20:01:00.793][MCR][E]mx_device.cpp            :2162: Failed creating memory
[20:01:00.793][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[20:01:00.793][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[20:01:00.793][MCR][E]mc_runtime_api_deprecated.cpp:306 : 425  : [7f3ce77986c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  57%|█████▋    | 17/30 [39:24<08:29, 39.16s/it]Loading checkpoint shards:  60%|██████    | 18/30 [39:30<11:30, 57.58s/it]Loading checkpoint shards:  63%|██████▎   | 19/30 [39:30<07:24, 40.38s/it]Loading checkpoint shards:  67%|██████▋   | 20/30 [39:30<04:43, 28.35s/it][20:01:10.684][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[20:01:10.932][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[20:01:10.957][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x6e487a0
[20:01:10.958][MCR][E]mx_device.cpp            :2162: Failed creating memory
[20:01:10.967][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[20:01:10.967][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[20:01:10.971][MCR][E]mc_runtime_api_deprecated.cpp:306 : 421  : [7f321ee506c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  60%|██████    | 18/30 [39:33<11:24, 57.03s/it]Loading checkpoint shards:  63%|██████▎   | 19/30 [39:33<07:19, 39.99s/it]Loading checkpoint shards:  67%|██████▋   | 20/30 [39:33<04:40, 28.08s/it][20:01:13.430][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[20:01:13.430][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[20:01:13.430][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x7c57590
[20:01:13.430][MCR][E]mx_device.cpp            :2162: Failed creating memory
[20:01:13.430][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[20:01:13.430][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68316823552 
[20:01:13.430][MCR][E]mc_runtime_api_deprecated.cpp:306 : 422  : [7f264126b6c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  60%|██████    | 18/30 [39:53<07:13, 36.09s/it]Loading checkpoint shards:  63%|██████▎   | 19/30 [40:21<06:11, 33.77s/it]Loading checkpoint shards:  67%|██████▋   | 20/30 [40:29<08:04, 48.49s/it]Loading checkpoint shards:  67%|██████▋   | 20/30 [40:29<08:05, 48.52s/it]Loading checkpoint shards:  70%|███████   | 21/30 [40:29<05:06, 34.05s/it]Loading checkpoint shards:  70%|███████   | 21/30 [40:30<05:06, 34.07s/it]Loading checkpoint shards:  73%|███████▎  | 22/30 [40:30<03:12, 24.04s/it]Loading checkpoint shards:  73%|███████▎  | 22/30 [40:30<03:12, 24.08s/it][20:02:10.746][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[20:02:10.746][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[20:02:10.746][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x78a8720
[20:02:10.746][MCR][E]mx_device.cpp            :2162: Failed creating memory
[20:02:10.746][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[20:02:10.746][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[20:02:10.746][MCR][E]mc_runtime_api_deprecated.cpp:306 : 419  : [7f6ca936d6c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
[20:02:10.750][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[20:02:10.750][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[20:02:10.750][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x783a5f0
[20:02:10.750][MCR][E]mx_device.cpp            :2162: Failed creating memory
[20:02:10.750][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[20:02:10.750][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[20:02:10.750][MCR][E]mc_runtime_api_deprecated.cpp:306 : 420  : [7fc29734f6c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  67%|██████▋   | 20/30 [40:39<08:11, 49.17s/it]Loading checkpoint shards:  70%|███████   | 21/30 [40:40<05:10, 34.51s/it]Loading checkpoint shards:  73%|███████▎  | 22/30 [40:40<03:14, 24.26s/it]Loading checkpoint shards:  77%|███████▋  | 23/30 [40:40<01:59, 17.08s/it][20:02:20.704][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[20:02:20.704][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[20:02:20.704][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x6cd3ee0
[20:02:20.704][MCR][E]mx_device.cpp            :2162: Failed creating memory
[20:02:20.704][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[20:02:20.704][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[20:02:20.704][MCR][E]mc_runtime_api_deprecated.cpp:306 : 424  : [7f5ca4f1f6c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  67%|██████▋   | 20/30 [40:49<05:19, 31.99s/it]Loading checkpoint shards:  70%|███████   | 21/30 [40:53<06:32, 43.63s/it]Loading checkpoint shards:  73%|███████▎  | 22/30 [40:54<04:05, 30.63s/it]Loading checkpoint shards:  77%|███████▋  | 23/30 [40:54<02:30, 21.53s/it][20:02:34.073][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[20:02:34.073][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[20:02:34.073][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x7b7f770
[20:02:34.073][MCR][E]mx_device.cpp            :2162: Failed creating memory
[20:02:34.073][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[20:02:34.073][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[20:02:34.073][MCR][E]mc_runtime_api_deprecated.cpp:306 : 423  : [7fd6e6bb86c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  70%|███████   | 21/30 [40:54<06:34, 43.81s/it]Loading checkpoint shards:  73%|███████▎  | 22/30 [40:54<04:06, 30.75s/it]Loading checkpoint shards:  77%|███████▋  | 23/30 [40:55<02:31, 21.62s/it][20:02:34.870][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[20:02:34.870][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[20:02:34.870][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x8027300
[20:02:34.870][MCR][E]mx_device.cpp            :2162: Failed creating memory
[20:02:34.870][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[20:02:34.870][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[20:02:34.870][MCR][E]mc_runtime_api_deprecated.cpp:306 : 425  : [7f3ce77986c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  70%|███████   | 21/30 [41:04<07:10, 47.85s/it]Loading checkpoint shards:  73%|███████▎  | 22/30 [41:04<04:28, 33.58s/it]Loading checkpoint shards:  77%|███████▋  | 23/30 [41:04<02:45, 23.60s/it][20:02:44.689][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[20:02:44.689][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[20:02:44.689][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x6e487a0
[20:02:44.689][MCR][E]mx_device.cpp            :2162: Failed creating memory
[20:02:44.689][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[20:02:44.689][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[20:02:44.689][MCR][E]mc_runtime_api_deprecated.cpp:306 : 421  : [7f321ee506c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  70%|███████   | 21/30 [41:07<07:09, 47.67s/it]Loading checkpoint shards:  73%|███████▎  | 22/30 [41:07<04:27, 33.46s/it]Loading checkpoint shards:  77%|███████▋  | 23/30 [41:08<02:45, 23.65s/it]Loading checkpoint shards:  80%|████████  | 24/30 [41:08<01:39, 16.64s/it][20:02:48.406][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[20:02:48.406][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 134217728 bytes from vram
[20:02:48.406][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x8000000, device 0x7c57590
[20:02:48.406][MCR][E]mx_device.cpp            :2162: Failed creating memory
[20:02:48.406][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[20:02:48.406][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :134217728 | free :0 | total :68316823552 
[20:02:48.406][MCR][E]mc_runtime_api_deprecated.cpp:306 : 422  : [7f264126b6c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  70%|███████   | 21/30 [41:17<04:36, 30.70s/it]Loading checkpoint shards:  73%|███████▎  | 22/30 [41:44<03:56, 29.51s/it]Loading checkpoint shards:  77%|███████▋  | 23/30 [42:03<05:14, 44.87s/it]Loading checkpoint shards:  77%|███████▋  | 23/30 [42:04<05:13, 44.84s/it]Loading checkpoint shards:  80%|████████  | 24/30 [42:04<03:09, 31.55s/it]Loading checkpoint shards:  80%|████████  | 24/30 [42:04<03:09, 31.53s/it]Loading checkpoint shards:  83%|████████▎ | 25/30 [42:04<01:51, 22.22s/it]Loading checkpoint shards:  83%|████████▎ | 25/30 [42:04<01:51, 22.21s/it]Loading checkpoint shards:  87%|████████▋ | 26/30 [42:05<01:03, 15.83s/it]Loading checkpoint shards:  87%|████████▋ | 26/30 [42:05<01:03, 15.84s/it][20:03:45.834][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[20:03:45.834][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[20:03:45.834][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x78a8720
[20:03:45.834][MCR][E]mx_device.cpp            :2162: Failed creating memory
[20:03:45.834][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[20:03:45.834][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[20:03:45.834][MCR][E]mc_runtime_api_deprecated.cpp:306 : 419  : [7f6ca936d6c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
[20:03:45.839][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[20:03:45.839][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[20:03:45.839][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x783a5f0
[20:03:45.839][MCR][E]mx_device.cpp            :2162: Failed creating memory
[20:03:45.839][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[20:03:45.839][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[20:03:45.839][MCR][E]mc_runtime_api_deprecated.cpp:306 : 420  : [7fc29734f6c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  77%|███████▋  | 23/30 [42:13<03:26, 29.52s/it]Loading checkpoint shards:  80%|████████  | 24/30 [42:14<04:01, 40.18s/it]Loading checkpoint shards:  83%|████████▎ | 25/30 [42:15<02:21, 28.23s/it]Loading checkpoint shards:  87%|████████▋ | 26/30 [42:15<01:19, 19.86s/it][20:03:55.261][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[20:03:55.261][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[20:03:55.261][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x6cd3ee0
[20:03:55.261][MCR][E]mx_device.cpp            :2162: Failed creating memory
[20:03:55.261][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[20:03:55.261][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[20:03:55.261][MCR][E]mc_runtime_api_deprecated.cpp:306 : 424  : [7f5ca4f1f6c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  80%|████████  | 24/30 [42:27<04:17, 42.96s/it]Loading checkpoint shards:  80%|████████  | 24/30 [42:27<04:17, 42.84s/it]Loading checkpoint shards:  83%|████████▎ | 25/30 [42:27<02:30, 30.17s/it]Loading checkpoint shards:  83%|████████▎ | 25/30 [42:27<02:30, 30.09s/it]Loading checkpoint shards:  87%|████████▋ | 26/30 [42:28<01:24, 21.22s/it][20:04:07.777][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[20:04:07.777][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[20:04:07.777][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x7b7f770
[20:04:07.777][MCR][E]mx_device.cpp            :2162: Failed creating memory
[20:04:07.777][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[20:04:07.777][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[20:04:07.777][MCR][E]mc_runtime_api_deprecated.cpp:306 : 423  : [7fd6e6bb86c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  87%|████████▋ | 26/30 [42:28<01:24, 21.17s/it][20:04:07.962][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[20:04:07.962][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[20:04:07.962][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x8027300
[20:04:07.962][MCR][E]mx_device.cpp            :2162: Failed creating memory
[20:04:07.962][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[20:04:07.962][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[20:04:07.962][MCR][E]mc_runtime_api_deprecated.cpp:306 : 425  : [7f3ce77986c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  80%|████████  | 24/30 [42:39<04:29, 44.97s/it]Loading checkpoint shards:  83%|████████▎ | 25/30 [42:39<02:37, 31.56s/it]Loading checkpoint shards:  87%|████████▋ | 26/30 [42:40<01:28, 22.18s/it]Loading checkpoint shards:  90%|█████████ | 27/30 [42:40<00:46, 15.62s/it][20:04:20.973][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[20:04:20.973][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[20:04:20.973][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x6e487a0
[20:04:20.973][MCR][E]mx_device.cpp            :2162: Failed creating memory
[20:04:20.973][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[20:04:20.973][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[20:04:20.973][MCR][E]mc_runtime_api_deprecated.cpp:306 : 421  : [7f321ee506c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  80%|████████  | 24/30 [42:42<02:56, 29.45s/it]Loading checkpoint shards:  83%|████████▎ | 25/30 [42:43<03:20, 40.12s/it]Loading checkpoint shards:  87%|████████▋ | 26/30 [42:43<01:52, 28.19s/it]Loading checkpoint shards:  90%|█████████ | 27/30 [42:44<00:59, 19.84s/it][20:04:23.738][MCR][E]mx_device.cpp            :2162: Failed creating memory
[20:04:23.738][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[20:04:23.738][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68316823552 
[20:04:23.738][MCR][E]mc_runtime_api_deprecated.cpp:306 : 422  : [7f264126b6c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  83%|████████▎ | 25/30 [43:09<02:23, 28.70s/it]Loading checkpoint shards:  87%|████████▋ | 26/30 [43:36<01:52, 28.16s/it]Loading checkpoint shards:  90%|█████████ | 27/30 [43:36<01:55, 38.43s/it]Loading checkpoint shards:  90%|█████████ | 27/30 [43:37<01:55, 38.51s/it]Loading checkpoint shards:  93%|█████████▎| 28/30 [43:37<00:54, 27.02s/it]Loading checkpoint shards:  93%|█████████▎| 28/30 [43:37<00:54, 27.07s/it]Loading checkpoint shards:  97%|█████████▋| 29/30 [43:38<00:19, 19.08s/it]Loading checkpoint shards:  97%|█████████▋| 29/30 [43:38<00:19, 19.13s/it][20:05:17.715][MCR][E]mx_device.cpp            :2162: Failed creating memory
[20:05:17.715][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[20:05:17.715][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :2101346304 | free :0 | total :68283269120 
[20:05:17.715][MCR][E]mc_runtime_api_deprecated.cpp:306 : 420  : [7fc29734f6c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
[20:05:17.735][MCR][E]mx_device.cpp            :2162: Failed creating memory
[20:05:17.735][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[20:05:17.735][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :2101346304 | free :0 | total :68283269120 
[20:05:17.735][MCR][E]mc_runtime_api_deprecated.cpp:306 : 419  : [7f6ca936d6c0] mcMalloc_v0: Returned mcErrorMemoryAllocation
Loading checkpoint shards:  90%|█████████ | 27/30 [43:47<02:04, 41.44s/it]Loading checkpoint shards:  93%|█████████▎| 28/30 [43:47<00:58, 29.12s/it]Loading checkpoint shards:  97%|█████████▋| 29/30 [43:48<00:20, 20.49s/it]Loading checkpoint shards: 100%|██████████| 30/30 [43:48<00:00, 14.41s/it]Loading checkpoint shards: 100%|██████████| 30/30 [43:48<00:00, 87.61s/it]
07/31/2024 20:05:27 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
07/31/2024 20:05:27 - INFO - llamafactory.model.model_utils.attention - Using FlashAttention-2 for faster training and inference.
07/31/2024 20:05:27 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
07/31/2024 20:05:27 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
07/31/2024 20:05:27 - INFO - llamafactory.model.model_utils.misc - Found linear modules: q_proj,v_proj,up_proj,down_proj,o_proj,gate_proj,k_proj
Loading checkpoint shards:  90%|█████████ | 27/30 [43:59<02:07, 42.35s/it]Loading checkpoint shards:  90%|█████████ | 27/30 [43:59<02:06, 42.31s/it]Loading checkpoint shards:  93%|█████████▎| 28/30 [44:00<00:59, 29.74s/it]Loading checkpoint shards:  93%|█████████▎| 28/30 [44:00<00:59, 29.72s/it]Loading checkpoint shards:  97%|█████████▋| 29/30 [44:00<00:20, 20.92s/it]Loading checkpoint shards:  97%|█████████▋| 29/30 [44:00<00:20, 20.90s/it]Loading checkpoint shards: 100%|██████████| 30/30 [44:00<00:00, 14.70s/it]Loading checkpoint shards: 100%|██████████| 30/30 [44:00<00:00, 88.02s/it]
07/31/2024 20:05:40 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
07/31/2024 20:05:40 - INFO - llamafactory.model.model_utils.attention - Using FlashAttention-2 for faster training and inference.
07/31/2024 20:05:40 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
07/31/2024 20:05:40 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
07/31/2024 20:05:40 - INFO - llamafactory.model.model_utils.misc - Found linear modules: k_proj,v_proj,gate_proj,o_proj,q_proj,up_proj,down_proj
Loading checkpoint shards: 100%|██████████| 30/30 [44:00<00:00, 14.69s/it]Loading checkpoint shards: 100%|██████████| 30/30 [44:00<00:00, 88.03s/it]
07/31/2024 20:05:40 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
07/31/2024 20:05:40 - INFO - llamafactory.model.model_utils.attention - Using FlashAttention-2 for faster training and inference.
07/31/2024 20:05:40 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
07/31/2024 20:05:40 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
07/31/2024 20:05:40 - INFO - llamafactory.model.model_utils.misc - Found linear modules: q_proj,k_proj,o_proj,v_proj,up_proj,gate_proj,down_proj
Loading checkpoint shards:  90%|█████████ | 27/30 [44:04<01:23, 27.95s/it]Loading checkpoint shards:  93%|█████████▎| 28/30 [44:14<01:18, 39.08s/it]Loading checkpoint shards:  97%|█████████▋| 29/30 [44:14<00:27, 27.44s/it]Loading checkpoint shards: 100%|██████████| 30/30 [44:14<00:00, 19.26s/it]Loading checkpoint shards: 100%|██████████| 30/30 [44:14<00:00, 88.50s/it]
07/31/2024 20:05:54 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
07/31/2024 20:05:54 - INFO - llamafactory.model.model_utils.attention - Using FlashAttention-2 for faster training and inference.
07/31/2024 20:05:54 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
07/31/2024 20:05:54 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
07/31/2024 20:05:54 - INFO - llamafactory.model.model_utils.misc - Found linear modules: down_proj,q_proj,gate_proj,v_proj,up_proj,k_proj,o_proj
Loading checkpoint shards:  93%|█████████▎| 28/30 [44:17<01:23, 41.84s/it]Loading checkpoint shards:  97%|█████████▋| 29/30 [44:17<00:29, 29.38s/it]Loading checkpoint shards: 100%|██████████| 30/30 [44:17<00:00, 20.62s/it]Loading checkpoint shards: 100%|██████████| 30/30 [44:17<00:00, 88.59s/it]
07/31/2024 20:05:57 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
07/31/2024 20:05:57 - INFO - llamafactory.model.model_utils.attention - Using FlashAttention-2 for faster training and inference.
07/31/2024 20:05:57 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
07/31/2024 20:05:57 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
07/31/2024 20:05:57 - INFO - llamafactory.model.model_utils.misc - Found linear modules: k_proj,q_proj,up_proj,v_proj,o_proj,gate_proj,down_proj
Loading checkpoint shards:  93%|█████████▎| 28/30 [44:33<00:56, 28.30s/it]Loading checkpoint shards:  97%|█████████▋| 29/30 [45:02<00:28, 28.42s/it]Loading checkpoint shards: 100%|██████████| 30/30 [45:02<00:00, 38.66s/it]Loading checkpoint shards: 100%|██████████| 30/30 [45:02<00:00, 90.08s/it]
Loading checkpoint shards: 100%|██████████| 30/30 [45:02<00:00, 38.69s/it]Loading checkpoint shards: 100%|██████████| 30/30 [45:02<00:00, 90.08s/it]
07/31/2024 20:06:42 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
07/31/2024 20:06:42 - INFO - llamafactory.model.model_utils.attention - Using FlashAttention-2 for faster training and inference.
07/31/2024 20:06:42 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
07/31/2024 20:06:42 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
07/31/2024 20:06:42 - INFO - llamafactory.model.model_utils.misc - Found linear modules: q_proj,down_proj,k_proj,up_proj,o_proj,v_proj,gate_proj
07/31/2024 20:06:42 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
07/31/2024 20:06:42 - INFO - llamafactory.model.model_utils.attention - Using FlashAttention-2 for faster training and inference.
07/31/2024 20:06:42 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
07/31/2024 20:06:42 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
07/31/2024 20:06:42 - INFO - llamafactory.model.model_utils.misc - Found linear modules: gate_proj,down_proj,v_proj,o_proj,up_proj,k_proj,q_proj
Loading checkpoint shards: 100%|██████████| 30/30 [45:14<00:00, 23.54s/it]Loading checkpoint shards: 100%|██████████| 30/30 [45:14<00:00, 90.48s/it]
[INFO|modeling_utils.py:4280] 2024-07-31 20:06:53,749 >> All model checkpoint weights were used when initializing LlamaForCausalLM.

[INFO|modeling_utils.py:4288] 2024-07-31 20:06:53,749 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /AI-DATA/Models/Llama/Meta-Llama-3-70B.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[INFO|configuration_utils.py:915] 2024-07-31 20:06:53,754 >> loading configuration file /AI-DATA/Models/Llama/Meta-Llama-3-70B/generation_config.json
[INFO|configuration_utils.py:962] 2024-07-31 20:06:53,755 >> Generate config GenerationConfig {
  "bos_token_id": 128000,
  "do_sample": true,
  "eos_token_id": 128001,
  "max_length": 4096,
  "temperature": 0.6,
  "top_p": 0.9
}

07/31/2024 20:06:53 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
07/31/2024 20:06:53 - INFO - llamafactory.model.model_utils.attention - Using FlashAttention-2 for faster training and inference.
07/31/2024 20:06:53 - INFO - llamafactory.model.adapter - ZeRO3 / FSDP detected, remaining trainable params in float32.
07/31/2024 20:06:53 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
07/31/2024 20:06:53 - INFO - llamafactory.model.model_utils.misc - Found linear modules: k_proj,q_proj,v_proj,up_proj,o_proj,gate_proj,down_proj
07/31/2024 20:06:54 - INFO - llamafactory.model.loader - trainable params: 103,546,880 || all params: 70,657,253,376 || trainable%: 0.1465
07/31/2024 20:06:54 - INFO - llamafactory.model.loader - trainable params: 103,546,880 || all params: 70,657,253,376 || trainable%: 0.1465
07/31/2024 20:06:54 - INFO - llamafactory.model.loader - trainable params: 103,546,880 || all params: 70,657,253,376 || trainable%: 0.1465
07/31/2024 20:06:54 - INFO - llamafactory.model.loader - trainable params: 103,546,880 || all params: 70,657,253,376 || trainable%: 0.1465
07/31/2024 20:06:54 - INFO - llamafactory.model.loader - trainable params: 103,546,880 || all params: 70,657,253,376 || trainable%: 0.1465
07/31/2024 20:06:54 - INFO - llamafactory.model.loader - trainable params: 103,546,880 || all params: 70,657,253,376 || trainable%: 0.1465
07/31/2024 20:06:54 - INFO - llamafactory.model.loader - trainable params: 103,546,880 || all params: 70,657,253,376 || trainable%: 0.1465
07/31/2024 20:06:54 - INFO - llamafactory.model.loader - trainable params: 103,546,880 || all params: 70,657,253,376 || trainable%: 0.1465
[INFO|trainer.py:641] 2024-07-31 20:06:54,887 >> Using auto half precision backend
07/31/2024 20:06:54 - WARNING - llamafactory.train.callbacks - Previous trainer log in this folder will be deleted.
[2024-07-31 20:06:55,292] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.3+unknown, git-hash=unknown, git-branch=unknown
[2024-07-31 20:06:55,344] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-07-31 20:06:55,350] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-07-31 20:06:55,350] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-07-31 20:06:55,497] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2024-07-31 20:06:55,497] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2024-07-31 20:06:55,497] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2024-07-31 20:06:55,497] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2024-07-31 20:06:55,705] [INFO] [utils.py:802:see_memory_usage] Stage 3 initialize beginning
[2024-07-31 20:06:55,705] [INFO] [utils.py:803:see_memory_usage] MA 16.62 GB         Max_MA 22.05 GB         CA 17.44 GB         Max_CA 25 GB 
[2024-07-31 20:06:55,705] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 62.11 GB, percent = 3.1%
[2024-07-31 20:06:55,718] [INFO] [stage3.py:127:__init__] Reduce bucket size 67108864
[2024-07-31 20:06:55,718] [INFO] [stage3.py:128:__init__] Prefetch bucket size 60397977
[2024-07-31 20:06:55,940] [INFO] [utils.py:802:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2024-07-31 20:06:55,940] [INFO] [utils.py:803:see_memory_usage] MA 16.62 GB         Max_MA 16.62 GB         CA 17.44 GB         Max_CA 17 GB 
[2024-07-31 20:06:55,941] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 62.18 GB, percent = 3.1%
Parameter Offload: Total persistent parameters: 49815552 in 1041 params
[2024-07-31 20:06:56,662] [INFO] [utils.py:802:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2024-07-31 20:06:56,662] [INFO] [utils.py:803:see_memory_usage] MA 16.45 GB         Max_MA 16.62 GB         CA 17.44 GB         Max_CA 17 GB 
[2024-07-31 20:06:56,662] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 62.28 GB, percent = 3.1%
[2024-07-31 20:06:56,863] [INFO] [utils.py:802:see_memory_usage] Before creating fp16 partitions
[2024-07-31 20:06:56,863] [INFO] [utils.py:803:see_memory_usage] MA 16.45 GB         Max_MA 16.45 GB         CA 17.44 GB         Max_CA 17 GB 
[2024-07-31 20:06:56,864] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 62.29 GB, percent = 3.1%
[2024-07-31 20:06:57,401] [INFO] [utils.py:802:see_memory_usage] After creating fp16 partitions: 1
[2024-07-31 20:06:57,402] [INFO] [utils.py:803:see_memory_usage] MA 16.45 GB         Max_MA 16.45 GB         CA 17.25 GB         Max_CA 17 GB 
[2024-07-31 20:06:57,402] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 62.27 GB, percent = 3.1%
[2024-07-31 20:06:57,600] [INFO] [utils.py:802:see_memory_usage] Before creating fp32 partitions
[2024-07-31 20:06:57,600] [INFO] [utils.py:803:see_memory_usage] MA 16.45 GB         Max_MA 16.45 GB         CA 17.25 GB         Max_CA 17 GB 
[2024-07-31 20:06:57,600] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 62.27 GB, percent = 3.1%
[2024-07-31 20:06:57,801] [INFO] [utils.py:802:see_memory_usage] After creating fp32 partitions
[2024-07-31 20:06:57,801] [INFO] [utils.py:803:see_memory_usage] MA 16.5 GB         Max_MA 16.52 GB         CA 17.25 GB         Max_CA 17 GB 
[2024-07-31 20:06:57,801] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 62.27 GB, percent = 3.1%
[2024-07-31 20:06:58,007] [INFO] [utils.py:802:see_memory_usage] Before initializing optimizer states
[2024-07-31 20:06:58,007] [INFO] [utils.py:803:see_memory_usage] MA 16.5 GB         Max_MA 16.5 GB         CA 17.25 GB         Max_CA 17 GB 
[2024-07-31 20:06:58,007] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 62.28 GB, percent = 3.1%
[2024-07-31 20:06:58,219] [INFO] [utils.py:802:see_memory_usage] After initializing optimizer states
[2024-07-31 20:06:58,219] [INFO] [utils.py:803:see_memory_usage] MA 16.6 GB         Max_MA 16.74 GB         CA 17.25 GB         Max_CA 17 GB 
[2024-07-31 20:06:58,220] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 62.28 GB, percent = 3.1%
[2024-07-31 20:06:58,220] [INFO] [stage3.py:479:_setup_for_real_optimizer] optimizer state initialized
[2024-07-31 20:06:58,877] [INFO] [utils.py:802:see_memory_usage] After initializing ZeRO optimizer
[2024-07-31 20:06:58,878] [INFO] [utils.py:803:see_memory_usage] MA 16.74 GB         Max_MA 16.75 GB         CA 17.31 GB         Max_CA 17 GB 
[2024-07-31 20:06:58,878] [INFO] [utils.py:810:see_memory_usage] CPU Virtual Memory:  used = 62.42 GB, percent = 3.1%
[2024-07-31 20:06:58,879] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = AdamW
[2024-07-31 20:06:58,879] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-07-31 20:06:58,879] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-07-31 20:06:58,879] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]
[2024-07-31 20:06:58,886] [INFO] [config.py:974:print] DeepSpeedEngine configuration:
[2024-07-31 20:06:59,370] [INFO] [config.py:978:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-07-31 20:06:59,371] [INFO] [config.py:978:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-07-31 20:06:59,378] [INFO] [config.py:978:print]   amp_enabled .................. False
[2024-07-31 20:06:59,379] [INFO] [config.py:978:print]   amp_params ................... False
[2024-07-31 20:06:59,387] [INFO] [config.py:978:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-07-31 20:06:59,388] [INFO] [config.py:978:print]   bfloat16_enabled ............. True
[2024-07-31 20:06:59,395] [INFO] [config.py:978:print]   checkpoint_parallel_write_pipeline  False
[2024-07-31 20:06:59,396] [INFO] [config.py:978:print]   checkpoint_tag_validation_enabled  True
[2024-07-31 20:06:59,403] [INFO] [config.py:978:print]   checkpoint_tag_validation_fail  False
[2024-07-31 20:06:59,404] [INFO] [config.py:978:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fbff42788b0>
[2024-07-31 20:06:59,412] [INFO] [config.py:978:print]   communication_data_type ...... None
[2024-07-31 20:06:59,413] [INFO] [config.py:978:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-07-31 20:06:59,420] [INFO] [config.py:978:print]   curriculum_enabled_legacy .... False
[2024-07-31 20:06:59,421] [INFO] [config.py:978:print]   curriculum_params_legacy ..... False
[2024-07-31 20:06:59,421] [INFO] [config.py:978:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-07-31 20:06:59,431] [INFO] [config.py:978:print]   data_efficiency_enabled ...... False
[2024-07-31 20:06:59,432] [INFO] [config.py:978:print]   dataloader_drop_last ......... False
[2024-07-31 20:06:59,439] [INFO] [config.py:978:print]   disable_allgather ............ False
[2024-07-31 20:06:59,440] [INFO] [config.py:978:print]   dump_state ................... False
[2024-07-31 20:06:59,447] [INFO] [config.py:978:print]   dynamic_loss_scale_args ...... None
[2024-07-31 20:06:59,449] [INFO] [config.py:978:print]   eigenvalue_enabled ........... False
[2024-07-31 20:06:59,456] [INFO] [config.py:978:print]   eigenvalue_gas_boundary_resolution  1
[2024-07-31 20:06:59,457] [INFO] [config.py:978:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-07-31 20:06:59,463] [INFO] [config.py:978:print]   eigenvalue_layer_num ......... 0
[2024-07-31 20:06:59,465] [INFO] [config.py:978:print]   eigenvalue_max_iter .......... 100
[2024-07-31 20:06:59,471] [INFO] [config.py:978:print]   eigenvalue_stability ......... 1e-06
[2024-07-31 20:06:59,472] [INFO] [config.py:978:print]   eigenvalue_tol ............... 0.01
[2024-07-31 20:06:59,479] [INFO] [config.py:978:print]   eigenvalue_verbose ........... False
[2024-07-31 20:06:59,480] [INFO] [config.py:978:print]   elasticity_enabled ........... False
[2024-07-31 20:06:59,487] [INFO] [config.py:978:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-07-31 20:06:59,488] [INFO] [config.py:978:print]   fp16_auto_cast ............... None
[2024-07-31 20:06:59,495] [INFO] [config.py:978:print]   fp16_enabled ................. False
[2024-07-31 20:06:59,496] [INFO] [config.py:978:print]   fp16_master_weights_and_gradients  False
[2024-07-31 20:06:59,503] [INFO] [config.py:978:print]   global_rank .................. 0
[2024-07-31 20:06:59,504] [INFO] [config.py:978:print]   grad_accum_dtype ............. None
[2024-07-31 20:06:59,511] [INFO] [config.py:978:print]   gradient_accumulation_steps .. 4
[2024-07-31 20:06:59,512] [INFO] [config.py:978:print]   gradient_clipping ............ 1.0
[2024-07-31 20:06:59,519] [INFO] [config.py:978:print]   gradient_predivide_factor .... 1.0
[2024-07-31 20:06:59,520] [INFO] [config.py:978:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-07-31 20:06:59,527] [INFO] [config.py:978:print]   initial_dynamic_scale ........ 1
[2024-07-31 20:06:59,528] [INFO] [config.py:978:print]   load_universal_checkpoint .... False
[2024-07-31 20:06:59,535] [INFO] [config.py:978:print]   loss_scale ................... 1.0
[2024-07-31 20:06:59,536] [INFO] [config.py:978:print]   memory_breakdown ............. False
[2024-07-31 20:06:59,543] [INFO] [config.py:978:print]   mics_hierarchial_params_gather  False
[2024-07-31 20:06:59,544] [INFO] [config.py:978:print]   mics_shard_size .............. -1
[2024-07-31 20:06:59,552] [INFO] [config.py:978:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-07-31 20:06:59,553] [INFO] [config.py:978:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-07-31 20:06:59,562] [INFO] [config.py:978:print]   optimizer_legacy_fusion ...... False
[2024-07-31 20:06:59,564] [INFO] [config.py:978:print]   optimizer_name ............... None
[2024-07-31 20:06:59,571] [INFO] [config.py:978:print]   optimizer_params ............. None
[2024-07-31 20:06:59,571] [INFO] [config.py:978:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-07-31 20:06:59,571] [INFO] [config.py:978:print]   pld_enabled .................. False
[2024-07-31 20:06:59,571] [INFO] [config.py:978:print]   pld_params ................... False
[2024-07-31 20:06:59,571] [INFO] [config.py:978:print]   prescale_gradients ........... False
[2024-07-31 20:06:59,571] [INFO] [config.py:978:print]   scheduler_name ............... None
[2024-07-31 20:06:59,571] [INFO] [config.py:978:print]   scheduler_params ............. None
[2024-07-31 20:06:59,571] [INFO] [config.py:978:print]   seq_parallel_communication_data_type  torch.float32
[2024-07-31 20:06:59,571] [INFO] [config.py:978:print]   sparse_attention ............. None
[2024-07-31 20:06:59,571] [INFO] [config.py:978:print]   sparse_gradients_enabled ..... False
[2024-07-31 20:06:59,571] [INFO] [config.py:978:print]   steps_per_print .............. inf
[2024-07-31 20:06:59,571] [INFO] [config.py:978:print]   train_batch_size ............. 32
[2024-07-31 20:06:59,571] [INFO] [config.py:978:print]   train_micro_batch_size_per_gpu  1
[2024-07-31 20:06:59,571] [INFO] [config.py:978:print]   use_node_local_storage ....... False
[2024-07-31 20:06:59,571] [INFO] [config.py:978:print]   wall_clock_breakdown ......... False
[2024-07-31 20:06:59,571] [INFO] [config.py:978:print]   weight_quantization_config ... None
[2024-07-31 20:06:59,571] [INFO] [config.py:978:print]   world_size ................... 8
[2024-07-31 20:06:59,571] [INFO] [config.py:978:print]   zero_allow_untested_optimizer  True
[2024-07-31 20:06:59,571] [INFO] [config.py:978:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=67108864 allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=60397977 param_persistence_threshold=81920 model_persistence_threshold=sys.maxsize max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-07-31 20:06:59,571] [INFO] [config.py:978:print]   zero_enabled ................. True
[2024-07-31 20:06:59,571] [INFO] [config.py:978:print]   zero_force_ds_cpu_optimizer .. True
[2024-07-31 20:06:59,571] [INFO] [config.py:978:print]   zero_optimization_stage ...... 3
[2024-07-31 20:06:59,571] [INFO] [config.py:964:print_user_config]   json = {
    "train_batch_size": 32, 
    "train_micro_batch_size_per_gpu": 1, 
    "gradient_accumulation_steps": 4, 
    "gradient_clipping": 1.0, 
    "zero_allow_untested_optimizer": true, 
    "fp16": {
        "enabled": false, 
        "loss_scale": 0, 
        "loss_scale_window": 1000, 
        "initial_scale_power": 16, 
        "hysteresis": 2, 
        "min_loss_scale": 1
    }, 
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 3, 
        "overlap_comm": true, 
        "contiguous_gradients": true, 
        "sub_group_size": 1.000000e+09, 
        "reduce_bucket_size": 6.710886e+07, 
        "stage3_prefetch_bucket_size": 6.039798e+07, 
        "stage3_param_persistence_threshold": 8.192000e+04, 
        "stage3_max_live_parameters": 1.000000e+09, 
        "stage3_max_reuse_distance": 1.000000e+09, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "steps_per_print": inf
}
[INFO|trainer.py:2078] 2024-07-31 20:06:59,571 >> ***** Running training *****
[INFO|trainer.py:2079] 2024-07-31 20:06:59,571 >>   Num examples = 131
[INFO|trainer.py:2080] 2024-07-31 20:06:59,571 >>   Num Epochs = 3
[INFO|trainer.py:2081] 2024-07-31 20:06:59,571 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2084] 2024-07-31 20:06:59,571 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:2085] 2024-07-31 20:06:59,572 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2086] 2024-07-31 20:06:59,572 >>   Total optimization steps = 12
[INFO|trainer.py:2087] 2024-07-31 20:06:59,582 >>   Number of trainable parameters = 103,546,880
  0%|          | 0/12 [00:00<?, ?it/s][20:07:13.331][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[20:07:13.331][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[20:07:13.331][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x7b7f770
[20:07:13.331][MCR][E]mx_device.cpp            :2162: Failed creating memory
[20:07:13.331][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[20:07:13.331][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[20:07:13.331][MCR][E]mc_runtime_api_deprecated.cpp:306 : 423  : [7fcfc97fa700] mcMalloc_v0: Returned mcErrorMemoryAllocation
[20:07:13.332][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[20:07:13.332][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[20:07:13.332][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x78a8720
[20:07:13.332][MCR][E]mx_device.cpp            :2162: Failed creating memory
[20:07:13.332][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[20:07:13.332][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[20:07:13.332][MCR][E]mc_runtime_api_deprecated.cpp:306 : 419  : [7f65937fe700] mcMalloc_v0: Returned mcErrorMemoryAllocation
[20:07:17.532][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[20:07:17.532][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[20:07:17.532][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x6cd3ee0
[20:07:17.532][MCR][E]mx_device.cpp            :2162: Failed creating memory
[20:07:17.532][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[20:07:17.532][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[20:07:17.532][MCR][E]mc_runtime_api_deprecated.cpp:306 : 424  : [7f55015fa700] mcMalloc_v0: Returned mcErrorMemoryAllocation
[20:07:24.225][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[20:07:24.225][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 469762048 bytes from vram
[20:07:24.225][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x1c000000, device 0x8027300
[20:07:24.225][MCR][E]mx_device.cpp            :2162: Failed creating memory
[20:07:24.225][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[20:07:24.225][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :469762048 | free :0 | total :68283269120 
[20:07:24.225][MCR][E]mc_runtime_api_deprecated.cpp:306 : 425  : [7f355bdff700] mcMalloc_v0: Returned mcErrorMemoryAllocation
[20:07:28.869][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[20:07:28.869][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 10485760 bytes from vram
[20:07:28.869][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0xa00000, device 0x783a5f0
[20:07:28.869][MCR][E]mx_device.cpp            :2162: Failed creating memory
[20:07:28.869][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[20:07:28.869][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :10485760 | free :0 | total :68283269120 
[20:07:28.869][MCR][E]mc_runtime_api.cpp       :940 : 420  : [7fba51dff700] mcExtMallocWithFlags: Returned mcErrorMemoryAllocation
[20:07:28.881][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[20:07:28.881][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 4194304 bytes from vram
[20:07:28.881][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x2bb418, device 0x783a5f0
[20:07:28.881][MCR][E]mx_device.cpp            :2162: Failed creating memory
[20:07:28.881][MCR][E]mc_recompile.cpp         :340 : Fail to allocate device memory
[20:07:28.881][MCR][E]mc_recompile.cpp         :366 : Fail to allocate device memory
[20:07:28.881][MCR][E]mc_platform.cpp          :604 : Fail to allocate device memory
[20:07:28.881][MCR][E]mc_runtime_api.cpp       :294 : 420  : [7fbb819fd700] mcLaunchKernel: Returned mcErrorInvalidValue
Traceback (most recent call last):
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1441, in wrapper
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2992, in reduce_scatter_tensor
    work = group._reduce_scatter_base(output, input, opts)
torch.distributed.DistBackendError: NCCL error in: /workspace/framework/mcPytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1275, internal error, NCCL version 2.16.5
ncclInternalError: Internal check failed.
Last error:
Proxy Call to rank 2 failed (Setup)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/software/temp/code/LLaMA-Factory-mainv0.8.3/src/llamafactory/launcher.py", line 23, in <module>
    launch()
  File "/software/temp/code/LLaMA-Factory-mainv0.8.3/src/llamafactory/launcher.py", line 19, in launch
    run_exp()
  File "/software/temp/code/LLaMA-Factory-mainv0.8.3/src/llamafactory/train/tuner.py", line 48, in run_exp
    run_pt(model_args, data_args, training_args, finetuning_args, callbacks)
  File "/software/temp/code/LLaMA-Factory-mainv0.8.3/src/llamafactory/train/pt/workflow.py", line 62, in run_pt
    train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
  File "/opt/conda/lib/python3.8/site-packages/transformers/trainer.py", line 1885, in train
    return inner_training_loop(
  File "/opt/conda/lib/python3.8/site-packages/transformers/trainer.py", line 2216, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/trainer.py", line 3250, in training_step
    self.accelerator.backward(loss)
  File "/opt/conda/lib/python3.8/site-packages/accelerate/accelerator.py", line 2143, in backward
    self.deepspeed_engine_wrapped.backward(loss, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/accelerate/utils/deepspeed.py", line 166, in backward
    self.engine.backward(loss, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/engine.py", line 1940, in backward
    self.optimizer.backward(loss, retain_graph=retain_graph)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/zero/stage3.py", line 2137, in backward
    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
    scaled_loss.backward(retain_graph=retain_graph)
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/function.py", line 274, in apply
    return user_fn(self, *args)
  File "/opt/conda/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 157, in backward
    torch.autograd.backward(outputs_with_grad, args_with_grad)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/zero/stage3.py", line 1119, in reduce_partition_and_remove_grads
    self.reduce_ready_partitions_and_remove_grads(param)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/zero/stage3.py", line 1411, in reduce_ready_partitions_and_remove_grads
    self.reduce_independent_p_g_buckets_and_remove_grads(param)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/zero/stage3.py", line 1154, in reduce_independent_p_g_buckets_and_remove_grads
    self.__reduce_and_partition_ipg_grads()
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/zero/stage3.py", line 1204, in __reduce_and_partition_ipg_grads
    grad_partitions = self.__avg_scatter_grads(self.params_in_ipg_bucket)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/zero/stage3.py", line 1273, in __avg_scatter_grads
    grad_partitions_for_rank = reduce_scatter_coalesced(full_grads_for_rank, self.dp_process_group)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/comm/coalesced_collectives.py", line 120, in reduce_scatter_coalesced
    _torch_reduce_scatter_fn(tensor_partition_flat_buffer,
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/comm/coalesced_collectives.py", line 23, in _torch_reduce_scatter_fn
    return instrument_w_nvtx(dist.reduce_scatter_fn)(output_tensor, input_tensor, group=group, async_op=False)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/comm/comm.py", line 257, in reduce_scatter_fn
    return reduce_scatter_tensor(output_tensor,
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/comm/comm.py", line 117, in log_wrapper
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/comm/comm.py", line 289, in reduce_scatter_tensor
    return cdb.reduce_scatter_tensor(output_tensor=output_tensor,
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/comm/torch.py", line 255, in reduce_scatter_tensor
    return self.reduce_scatter_function(output_tensor,
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1446, in wrapper
    "args": f"{args}, {kwargs}",
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 426, in __repr__
    return torch._tensor_str._str(self, tensor_contents=tensor_contents)
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor_str.py", line 636, in _str
    return _str_intern(self, tensor_contents=tensor_contents)
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor_str.py", line 567, in _str_intern
    tensor_str = _tensor_str(self, indent)
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor_str.py", line 327, in _tensor_str
    formatter = _Formatter(get_summarized_data(self) if summarize else self)
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor_str.py", line 116, in __init__
    tensor_view, torch.isfinite(tensor_view) & tensor_view.ne(0)
RuntimeError: CUDA error: invalid argument
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing MACA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

[20:07:28.935][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[20:07:28.935][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 10485760 bytes from vram
[20:07:28.935][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0xa00000, device 0x6e487a0
[20:07:28.935][MCR][E]mx_device.cpp            :2162: Failed creating memory
[20:07:28.935][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[20:07:28.935][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :10485760 | free :0 | total :68283269120 
[20:07:28.935][MCR][E]mc_runtime_api.cpp       :940 : 421  : [7f2a70bff700] mcExtMallocWithFlags: Returned mcErrorMemoryAllocation
[20:07:28.976][MXKW][E]fmm.c                   :1414: [__fmm_allocate_device_svm]Memory alloc failed 
[20:07:28.976][MXKW][E]memory.c                :261 : [mxkwAllocMemory]failed to allocate 268435456 bytes from vram
[20:07:28.976][MCR][E]mx_device.cpp            :1866: Fail allocation local memory, size 0x10000000, device 0x6e487a0
[20:07:28.976][MCR][E]mx_device.cpp            :2162: Failed creating memory
[20:07:28.976][MCR][E]mx_memory.cpp            :342 : Unable to allocate aligned memory
[20:07:28.976][MCR][E]mc_memory.cpp            :533 : Allocation failed : Device memory : required :268435456 | free :0 | total :68283269120 
[20:07:28.976][MCR][E]mc_runtime_api_deprecated.cpp:306 : 421  : [7f2b167fc700] mcMalloc_v0: Returned mcErrorMemoryAllocation
Traceback (most recent call last):
  File "/software/temp/code/LLaMA-Factory-mainv0.8.3/src/llamafactory/launcher.py", line 23, in <module>
    launch()
  File "/software/temp/code/LLaMA-Factory-mainv0.8.3/src/llamafactory/launcher.py", line 19, in launch
    run_exp()
  File "/software/temp/code/LLaMA-Factory-mainv0.8.3/src/llamafactory/train/tuner.py", line 48, in run_exp
    run_pt(model_args, data_args, training_args, finetuning_args, callbacks)
  File "/software/temp/code/LLaMA-Factory-mainv0.8.3/src/llamafactory/train/pt/workflow.py", line 62, in run_pt
    train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
  File "/opt/conda/lib/python3.8/site-packages/transformers/trainer.py", line 1885, in train
    return inner_training_loop(
  File "/opt/conda/lib/python3.8/site-packages/transformers/trainer.py", line 2216, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/trainer.py", line 3250, in training_step
    self.accelerator.backward(loss)
  File "/opt/conda/lib/python3.8/site-packages/accelerate/accelerator.py", line 2143, in backward
    self.deepspeed_engine_wrapped.backward(loss, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/accelerate/utils/deepspeed.py", line 166, in backward
    self.engine.backward(loss, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/engine.py", line 1940, in backward
    self.optimizer.backward(loss, retain_graph=retain_graph)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/zero/stage3.py", line 2137, in backward
    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
    scaled_loss.backward(retain_graph=retain_graph)
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/function.py", line 274, in apply
    return user_fn(self, *args)
  File "/opt/conda/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 157, in backward
    torch.autograd.backward(outputs_with_grad, args_with_grad)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/zero/stage3.py", line 1119, in reduce_partition_and_remove_grads
    self.reduce_ready_partitions_and_remove_grads(param)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/zero/stage3.py", line 1411, in reduce_ready_partitions_and_remove_grads
    self.reduce_independent_p_g_buckets_and_remove_grads(param)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/zero/stage3.py", line 1154, in reduce_independent_p_g_buckets_and_remove_grads
    self.__reduce_and_partition_ipg_grads()
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/zero/stage3.py", line 1204, in __reduce_and_partition_ipg_grads
    grad_partitions = self.__avg_scatter_grads(self.params_in_ipg_bucket)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/zero/stage3.py", line 1273, in __avg_scatter_grads
    grad_partitions_for_rank = reduce_scatter_coalesced(full_grads_for_rank, self.dp_process_group)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/comm/coalesced_collectives.py", line 120, in reduce_scatter_coalesced
    _torch_reduce_scatter_fn(tensor_partition_flat_buffer,
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/comm/coalesced_collectives.py", line 23, in _torch_reduce_scatter_fn
    return instrument_w_nvtx(dist.reduce_scatter_fn)(output_tensor, input_tensor, group=group, async_op=False)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/comm/comm.py", line 257, in reduce_scatter_fn
    return reduce_scatter_tensor(output_tensor,
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/comm/comm.py", line 117, in log_wrapper
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/comm/comm.py", line 289, in reduce_scatter_tensor
    return cdb.reduce_scatter_tensor(output_tensor=output_tensor,
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/comm/torch.py", line 255, in reduce_scatter_tensor
    return self.reduce_scatter_function(output_tensor,
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1441, in wrapper
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2992, in reduce_scatter_tensor
    work = group._reduce_scatter_base(output, input, opts)
torch.distributed.DistBackendError: NCCL error in: /workspace/framework/mcPytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1275, internal error, NCCL version 2.16.5
ncclInternalError: Internal check failed.
Last error:
Proxy Call to rank 3 failed (Setup)
Traceback (most recent call last):
  File "/software/temp/code/LLaMA-Factory-mainv0.8.3/src/llamafactory/launcher.py", line 23, in <module>
    launch()
  File "/software/temp/code/LLaMA-Factory-mainv0.8.3/src/llamafactory/launcher.py", line 19, in launch
    run_exp()
  File "/software/temp/code/LLaMA-Factory-mainv0.8.3/src/llamafactory/train/tuner.py", line 48, in run_exp
    run_pt(model_args, data_args, training_args, finetuning_args, callbacks)
  File "/software/temp/code/LLaMA-Factory-mainv0.8.3/src/llamafactory/train/pt/workflow.py", line 62, in run_pt
    train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
  File "/opt/conda/lib/python3.8/site-packages/transformers/trainer.py", line 1885, in train
    return inner_training_loop(
  File "/opt/conda/lib/python3.8/site-packages/transformers/trainer.py", line 2216, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/trainer.py", line 3250, in training_step
    self.accelerator.backward(loss)
  File "/opt/conda/lib/python3.8/site-packages/accelerate/accelerator.py", line 2143, in backward
    self.deepspeed_engine_wrapped.backward(loss, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/accelerate/utils/deepspeed.py", line 166, in backward
    self.engine.backward(loss, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/engine.py", line 1940, in backward
    self.optimizer.backward(loss, retain_graph=retain_graph)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/zero/stage3.py", line 2137, in backward
    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
    scaled_loss.backward(retain_graph=retain_graph)
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/function.py", line 274, in apply
    return user_fn(self, *args)
  File "/opt/conda/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 157, in backward
    torch.autograd.backward(outputs_with_grad, args_with_grad)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/zero/stage3.py", line 1119, in reduce_partition_and_remove_grads
    self.reduce_ready_partitions_and_remove_grads(param)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/zero/stage3.py", line 1411, in reduce_ready_partitions_and_remove_grads
    self.reduce_independent_p_g_buckets_and_remove_grads(param)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/zero/stage3.py", line 1154, in reduce_independent_p_g_buckets_and_remove_grads
    self.__reduce_and_partition_ipg_grads()
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/zero/stage3.py", line 1204, in __reduce_and_partition_ipg_grads
    grad_partitions = self.__avg_scatter_grads(self.params_in_ipg_bucket)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/zero/stage3.py", line 1273, in __avg_scatter_grads
    grad_partitions_for_rank = reduce_scatter_coalesced(full_grads_for_rank, self.dp_process_group)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/comm/coalesced_collectives.py", line 120, in reduce_scatter_coalesced
    _torch_reduce_scatter_fn(tensor_partition_flat_buffer,
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/comm/coalesced_collectives.py", line 23, in _torch_reduce_scatter_fn
    return instrument_w_nvtx(dist.reduce_scatter_fn)(output_tensor, input_tensor, group=group, async_op=False)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/comm/comm.py", line 257, in reduce_scatter_fn
    return reduce_scatter_tensor(output_tensor,
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/comm/comm.py", line 117, in log_wrapper
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/comm/comm.py", line 289, in reduce_scatter_tensor
    return cdb.reduce_scatter_tensor(output_tensor=output_tensor,
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/comm/torch.py", line 255, in reduce_scatter_tensor
    return self.reduce_scatter_function(output_tensor,
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1441, in wrapper
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2992, in reduce_scatter_tensor
    work = group._reduce_scatter_base(output, input, opts)
torch.distributed.DistBackendError: NCCL error in: /workspace/framework/mcPytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1275, internal error, NCCL version 2.16.5
ncclInternalError: Internal check failed.
Last error:
socketStartConnect: Connect to 10.2.179.27<35279> failed : Software caused connection abort
Traceback (most recent call last):
  File "/software/temp/code/LLaMA-Factory-mainv0.8.3/src/llamafactory/launcher.py", line 23, in <module>
    launch()
  File "/software/temp/code/LLaMA-Factory-mainv0.8.3/src/llamafactory/launcher.py", line 19, in launch
    run_exp()
  File "/software/temp/code/LLaMA-Factory-mainv0.8.3/src/llamafactory/train/tuner.py", line 48, in run_exp
    run_pt(model_args, data_args, training_args, finetuning_args, callbacks)
  File "/software/temp/code/LLaMA-Factory-mainv0.8.3/src/llamafactory/train/pt/workflow.py", line 62, in run_pt
    train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)
  File "/opt/conda/lib/python3.8/site-packages/transformers/trainer.py", line 1885, in train
    return inner_training_loop(
  File "/opt/conda/lib/python3.8/site-packages/transformers/trainer.py", line 2216, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/opt/conda/lib/python3.8/site-packages/transformers/trainer.py", line 3250, in training_step
    self.accelerator.backward(loss)
  File "/opt/conda/lib/python3.8/site-packages/accelerate/accelerator.py", line 2143, in backward
    self.deepspeed_engine_wrapped.backward(loss, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/accelerate/utils/deepspeed.py", line 166, in backward
    self.engine.backward(loss, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/engine.py", line 1940, in backward
    self.optimizer.backward(loss, retain_graph=retain_graph)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/zero/stage3.py", line 2137, in backward
    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/fp16/loss_scaler.py", line 63, in backward
    scaled_loss.backward(retain_graph=retain_graph)
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/function.py", line 274, in apply
    return user_fn(self, *args)
  File "/opt/conda/lib/python3.8/site-packages/torch/utils/checkpoint.py", line 157, in backward
    torch.autograd.backward(outputs_with_grad, args_with_grad)
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/zero/stage3.py", line 1119, in reduce_partition_and_remove_grads
    self.reduce_ready_partitions_and_remove_grads(param)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/zero/stage3.py", line 1411, in reduce_ready_partitions_and_remove_grads
    self.reduce_independent_p_g_buckets_and_remove_grads(param)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/zero/stage3.py", line 1154, in reduce_independent_p_g_buckets_and_remove_grads
    self.__reduce_and_partition_ipg_grads()
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/zero/stage3.py", line 1204, in __reduce_and_partition_ipg_grads
    grad_partitions = self.__avg_scatter_grads(self.params_in_ipg_bucket)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/zero/stage3.py", line 1273, in __avg_scatter_grads
    grad_partitions_for_rank = reduce_scatter_coalesced(full_grads_for_rank, self.dp_process_group)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/comm/coalesced_collectives.py", line 120, in reduce_scatter_coalesced
    _torch_reduce_scatter_fn(tensor_partition_flat_buffer,
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/runtime/comm/coalesced_collectives.py", line 23, in _torch_reduce_scatter_fn
    return instrument_w_nvtx(dist.reduce_scatter_fn)(output_tensor, input_tensor, group=group, async_op=False)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/utils/nvtx.py", line 15, in wrapped_fn
    ret_val = func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/comm/comm.py", line 257, in reduce_scatter_fn
    return reduce_scatter_tensor(output_tensor,
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/comm/comm.py", line 117, in log_wrapper
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/comm/comm.py", line 289, in reduce_scatter_tensor
    return cdb.reduce_scatter_tensor(output_tensor=output_tensor,
  File "/opt/conda/lib/python3.8/site-packages/deepspeed/comm/torch.py", line 255, in reduce_scatter_tensor
    return self.reduce_scatter_function(output_tensor,
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 1441, in wrapper
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py", line 2992, in reduce_scatter_tensor
    work = group._reduce_scatter_base(output, input, opts)
torch.distributed.DistBackendError: NCCL error in: /workspace/framework/mcPytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1275, internal error, NCCL version 2.16.5
ncclInternalError: Internal check failed.
Last error:
socketStartConnect: Connect to 10.2.179.27<39341> failed : Software caused connection abort
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 418 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 423 closing signal SIGTERM

Ctrl-C is pressed!
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 424 closing signal SIGTERM

Ctrl-C is pressed!
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 425 closing signal SIGTERM

Ctrl-C is pressed!

Ctrl-C is pressed!
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 419) of binary: /opt/conda/bin/python
Traceback (most recent call last):
  File "/opt/conda/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
/software/temp/code/LLaMA-Factory-mainv0.8.3/src/llamafactory/launcher.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2024-07-31_20:07:36
  host      : node4
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 420)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2024-07-31_20:07:36
  host      : node4
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 421)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2024-07-31_20:07:36
  host      : node4
  rank      : 4 (local_rank: 4)
  exitcode  : 1 (pid: 422)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-07-31_20:07:36
  host      : node4
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 419)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
